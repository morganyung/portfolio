{"cells":[{"cell_type":"markdown","source":["# FP_Section2_Group2_Phase4_MASTER\n\nDATABRICKS URL: https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/942460095360751/command/942460095360752"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"29cb3d70-5428-498e-b62d-61206452c7b2","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Team and project meta information"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d3d46f5d-7c09-4752-852b-bf42cc1a7398","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Team Information"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ac73fc06-6d01-4c64-afa3-93d424a12bf3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<img src='https://drive.google.com/uc?id=1NeC3pWdYj4FjAc4500ry7390MKfJ3-v1' alt='Google Drive Image' width=50%/>\n\nProject Title: Predicting Flight Delays to Reduce Airline Economic Loss <br>\nGroup Number: Section 2 Group 2 <br>\nTeam Name: Mars <br>\nTeam Members Morgan Raymond Fang, Audrey Phone, Sophey Yeh, Morgan Yung"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1e4a14c-5798-4bc0-9475-61693c74386e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Contact Information\n|  | Name | Email | \n| ----------- | ----------- | ----------- |\n| 1      | Audrey Phone    | audreyphone@berkeley.edu |\n| 2   | Raymond Fang        | rafang25@berkeley.edu |\n| 3   | Morgan Yung        | my4223@berkeley.edu |\n| 4   | Sophie Yeh      | syeh1@berkeley.edu |"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4f60222c-0dd5-4273-992e-0361226ef8c4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Credit Assignments\n| Task     | Assignment | Assignee | Start Date | Due Date | Time Allotment |Priority |\n| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\n| 2   | EDA | Group | 11-28 | 11-29 | 240 Min | H |\n| 3   | Feature Engineering | Group | 11-29 | 11-30 | 420 Min | H |\n| 4   | Data Lineage | Raymond F. | 11-29 | 11-30 | 120 Min | M |\n| 5   | Neural Network P1 | Sophie Y. | 11-30 | 12-1 | 120 Min | M |\n| 6   | Grid Search Implementation |  Group | 11-30 | 12-1 | 60 Min | H |\n| 7   | Build Evaluate Functions | Sophie Y. | 11-30 | 12-1 | 180 Min | H |\n| 8   | Implement Early Stop |  Morgan Y. | 12-2 | 12-3| 300 Min | M |\n| 9   | Decide Novel direction | Ray. F & Audrey P. | 12-2 | 12-3 | 30 Min | M |\n| 10  | SMOTE | Ray. F & Audrey P.| 12-2 | 12-3 | 720 Min | H |\n| 11  | Experiments | Group | 12-2 | 12-3 |180 Min |  M |\n| 12  | Develop Neural Network Architecture | Sophie Y. | 12-2 | 12-3 |  120 Min | M |\n| 13  | Update Project Leaderboard w/ results | Sophie Y. | 12-3 |12-4 |  15 Min | L |\n| 14  | Clean up Code | Group | 12-3 | 12-4 | 240 Min | H |\n| 15  | Clean up Final Report | Group | 12-3 | 12-4  | 240 Min | H |"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6955744c-237c-4a1c-9e61-58d05cedb72b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Thesis\n\nMany factors impact the predition of flight delays. Features ranging from weather, time, and flight related features all play a role in determining the outcome of a flight delay."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d6245ca0-f9f2-4c01-a30f-8433c15ddcc5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Abstract \n\nFlight delays have been an increasingly common problem for airlines over the last decade. These delays can lead to cascading effects of additional delays, and even worse flight cancellations. Being able to better predict flight delays will allow operational optimizations and a reduction in economic losses. The goal of our project is to create a machine learning model that predicts flight departure delays 2 hours ahead of time as a tool for airlines to improve flight scheduling and better respond to such delays. The focus of Phase 4 is to introduced more complex models as well as additional features that would help improve the predictive power of our model. In this phase, we introduced four new features, developed a multilayered neural network, and applied SMOTE to handle our feature imbalance concerns. Our results indicate that MNN performed the best with a test weighted F1 score of 0.7651."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"041430f2-aed9-46cd-8dda-785bbc25eb57","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Introduction"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4e94da70-4f6b-4395-9de8-109540b8c825","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Business Case\n\nFlight delays have been an increasingly common problem for airlines over the last decade. These delays can lead to cascading effects of additional delays, and even worse flight cancellations. Being able to better predict flight delays will allow operational optimizations and a reduction in economic losses. The goal of our project is to create a machine learning model that will aid in predicting flight departure delays 2 hours ahead of time as a tool for airlines to improve flight scheduling and better respond to such delays. We define delays as flights departing more than 15 minutes later than the scheduled time. By providing insights on flight delays, we hope to improve overall customer satisfaction, leading to a higher rate of returning customers and revenue for airlines. The target focus of this study are the customers purchasing flights from Airlines. We will measure the success of our Machine Learning algorithm by measuring the F1 score."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"565aa0cd-6b2b-44d4-af0e-1fb820c4f7f1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##Datasets\n<br>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a2b524c8-9c5f-4cc6-b2bc-21b8f7650638","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Flights Dataset\n\nThe flight dataset was downloaded from the US Department of Transportation and contains flight information from 2015 to 2021. The full airlines data has ~74M rows and 111 columns. The dataset consists of flights that were flown across the United States from 2014 - 2021. The flights are evenly distributed across time, but there are major class imbalances when it comes to delayed versus non-delayed flights. There are about 4.8x more non-delayed flights compared to delayed flights. The label in question, DEP_DEL15 also does not seem to have much correlation with most other features in this dataset. We will be relying on newly introduced features, feature engineered or otherwise.<br>\n\n<img src='https://drive.google.com/uc?id=1P6u26FO4rKcvHLr53YgEQFE3dLyYsR1i' alt='Google Drive Image' width=50%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"daf5d3d6-0d02-45bd-82c4-3daef7331bfc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Stations\n\nThe dataset displays a list of all stations with each record identify the station and it’s distance relative to its neighbor. Overall, there are 2,202 unique station names spread across North America, but based on their latitude and longitude, there are 2,225 unique stations.\n\nWhile the dataset did provide us with a handful of useful information on the stations, some information was only provided to the neighboring stations such as the state and call IDs. To populate this information, a filter was placed on the dataset where the distance_to_neighbor was equal to zero, implying the neighbor is itself. The data is then joined back on the stations dataset to provide a more wholistic picture of where stations resided.\n\nBased on the map of North America below, the states on the central - eastern regions of the United States have a much denser population of stations compared to that of the western side of the country.\n\n<img src='https://drive.google.com/uc?id=1z2VvNc2AckJ7f65pq4ixmo89LNw-dZrC' alt='Google Drive Image' width=50%/>\n\nHowever, that is not to say there is a weak presence on the western side of the country. On the contrary, of the top five states with the most stations, two of them are on the western side, Alaska and California. They simply are not as densely populated with stations due to the sheer size of the territories.\n\n<img src='https://drive.google.com/uc?id=1lj4Kjghz0_zRfkg7PbGM91aHx0bWRlM5' alt='Google Drive Image' width=50%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6383ede6-2e5c-43d8-ae3c-4dc38346becd","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Weather \nGeoplot shows that weather data set includes data from all around the world. Our primary focus though will only be on weather data in the US.\nThe weather dataset has many null values for various different columns. The null columns varied and some observations had varying different columns that were null and were inconsistent. Not all data followed a normal distribution. Analysis was primarily done on a subset based on prior knowledge. Hourly measurement data was the primary variables looked at for the exploratory data analysis since flights happen on an hourly basis. Histograms were used to identify inaccurate values as well as analyze the distribution of variables as shown below. \n\n\nRelative Humidity in particular did not follow a normal distribution. Wind direction was disregarded due to being a more ordinal variable.\nOf these variables, elevation and hourly wind speed had inaccurate values that will be filtered out in the final data set. The distribution of wind speed followed a normal distribution after removing aforementioned values as shown in the image below."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35014fce-f107-4085-a76d-189c423fa44f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<img src='https://drive.google.com/uc?id=168izQ730KGQC5Y1c2B9667JsCTOtJU2O' alt='Google Drive Image' width=50%/>\n\n\n<img src='https://drive.google.com/uc?id=18gn8UWICT_FZxuU6FIKED_GTLWhMjkJ7' alt='Google Drive Image' width=50%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9eee4c3f-1ebf-4d9a-999d-0512de718dd8","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Final Dataset\n\nWe took the three datasets above and joined them together, using the Flights dataset as our base. We proceeded to introduce additional features ranging from Time Series features such as time of the Flights as well as Graph Based features like Page Rank. We  introduced features such as event based (holidays), nature based (natural disasters) and airport related (reputation & maintenance) features. We also reduced the number of features by means of a PCA and reduced the dataset by removing any cancelled flights and some null records.\n\nThe final dataset came out to a dataset with ~41M records and 172 features. The final dataset will eventually be reduced further to the final features that provided the most predicting power."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cb239d41-1f85-4756-886d-dd82f0081deb","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Imports & Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dd70f302-2ec4-47f3-9d65-460aa82aa673","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Python packages\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom datetime import timedelta\npd.set_option('max_columns', None)\nimport time\n# Pyspark\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col,isnan,when,count,array\nfrom pyspark.sql.types import IntegerType \nfrom pyspark.sql.types import TimestampType, DateType, StringType, NumericType\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.sql import Window as W\nfrom pyspark import SparkConf, SparkContext\nimport random\nimport numpy as np\nfrom functools import reduce\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import rand,col,when,concat,substring,lit,udf,lower,sum as ps_sum,count as ps_count,row_number\nfrom pyspark.sql.window import *\nfrom pyspark.sql import DataFrame\nfrom pyspark.ml.feature import VectorAssembler,BucketedRandomProjectionLSH,VectorSlicer,StringIndexer,OneHotEncoder,StandardScaler\nfrom pyspark.ml.linalg import Vectors,VectorUDT, SparseVector\nfrom pyspark.sql.functions import array, create_map, struct\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import MultilayerPerceptronClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTEENN\n\nblob_container = \"w261\" # The name of your container created in https://portal.azure.com\nstorage_account = \"syeh\" # The name of your Storage account created in https://portal.azure.com\nsecret_scope = \"w261\" # The name of the scope created in your local computer using the Databricks CLI\nsecret_key = \"w261\" # saskey The name of the secret key created in your local computer using the Databricks CLI \nblob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\nmount_path = \"/mnt/mids-w261\"\n\nspark.conf.set(\n  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4e761db2-a6d2-4be5-94da-2c17e29bf82c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["file = f\"joined_df_time/\"\ndf_past = spark.read.parquet(f\"{blob_url}/{file}\")\ndf_2022 = spark.read.parquet(f\"{blob_url}/flights_2022/\")\ndf_2014 = spark.read.parquet(f\"{blob_url}/flights_2014/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"946542cc-2e84-46b7-8c03-dfb58dd1d604","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Data Preparation"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e567bd55-1755-427d-9ea8-6a9931af14cc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Data Lineage \n\nWe start our process by intaking the datasets mentioned above and create a joined dataset. As mentioned before, the flights dataset is the core for our final dataset. We joined Flights to Airport (Oirgin <> IATA), Airports to Stations (ICAO <> call), Weather to Stations (station_id <> station_id), and finally Weather back to Flights (CRS_DEP_DATETIME <> lag_time)\n\n<br><img src='https://drive.google.com/uc?id=1QNW6RJ9DqmJm_j1IZz66PC34FzbigXZc' alt='Google Drive Image' width=100%/>\n\nWe took this dataset and removed any flights that would not be valid in our analysis, primarily cancelled flights. \n\nWe then perform EDA on the joined dataset. While we went over this in detail in Phases 1 - 3, we opted to perform additional EDA on flights that took place in 2022. We were curious if there were any trends from the start of the COVID-19 pandemic and if there were any trends that persisted through 2022.\n\nNext, we took our findings from the EDA and perform our feature engineering step(s). This step includes adding in features including: graph based, time-series based, and miscellaneous features such as events taking place, natural disasters, and historical features of the airport (reputation, history of maintenance delays, etc). The dataset is then processed further by applying dimension reduction techniques like a PCA. An additional year of data (2014) was also added in to supplement some of the feature engineer process.\n\nThe next step is to take this curated dataset and enter our Data Preperation phase. A SMOTE technique is applied to handle class imbalance issues. To handle categorical features, they will be passed through a string indexer and then one-hot encoded. After this is complete, the features will be passed through a vector assembler and scaled accordingly. <br>\n\nFinally, we will pass the curated dataset through ML three models: Logistic Regression, Decision Tree, and a Multi-Layer Neural Network (MNN). The metric we will be scoring these models on is the F1 Score.\n<br><img src='https://drive.google.com/uc?id=1YUsCkO22pUcVvV5mqm6Tzpn1mLBvIEyc' alt='Google Drive Image' width=100%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7c5288b4-40bf-4216-a18f-37b210b27efd","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## 2019 - 2022 EDA"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8aa46a40-58b0-45ac-9fe2-fab2d2e64604","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["For this phase of the project, in addition to the data we already have, we added the flights from 2022. For our EDA, we will be looking at 2019, 2020, and 2022, so that we can capture the trends pre and post-pandemic. We analyzed the number of flights by month and it looks like the 2022 trend is very similar to that of 2019. <br>\n\n<img src='https://drive.google.com/uc?id=1hRhl1SMRjM3WIoEBgbAPbDkq23voq2C6' alt='Google Drive Image' width=50%/>\n\nWithin those flights, we looked at the ones that were on-time versus delayed. There is still a greater number of on-time flights, but within those that were delayed, 2020 showed a drop in delayed flights, perhaps due to the COVID-19 pandemic, while 2019 and 2022 showed similar proportions. <br>\n\n<img src='https://drive.google.com/uc?id=1vDXSxwNaO6O2y_LtYD9i3rd4UdcwaPTy' alt='Google Drive Image' width=50%/>\n\nBelow is a graph based on the length of the delay. For all three years, it seems like a majority of the flights left either before or on-time. <br>\n\n<img src='https://drive.google.com/uc?id=1TB03Q3_Uz0HMQ6zTBZcotDtvsitY8OAV' alt='Google Drive Image' width=50%/>\n\nWe then looked into the specific airlines and found that certain carriers had more flights, and in turn generally had more delays. Depending on the airline, the number of on-time versus delayed flights seem to follow the same trend, with the exception of the delayed flights in 2020, which is probably due to the pandemic and reduced number of flights. <br>\n\n<img src='https://drive.google.com/uc?id=11CAlkI7FAZK-Adr9Owb-gCH9Nplx3eBv' alt='Google Drive Image' width=50%/>\n<img src='https://drive.google.com/uc?id=1Vncqc3bq4LrkhSUIXKBpoYRvY-pTNQRY' alt='Google Drive Image' width=50%/>\n\nLastly, we looked the percentage of delays to over the three years and compared to 2019, 2022 seems to have a higher percentage of flights that were delayed versus on-time. <br>\n\n<img src='https://drive.google.com/uc?id=1Fcpp0S7eoAjGebRMO6r0ujDnzsBeZ-OI' alt='Google Drive Image' width=50%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"45c6edf7-bb67-437d-85ad-da1ac9fe8c45","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Feature Engineering\n\nDuring the feature engineering phase, we realize we have a need for additional features and a need for feature reduction. We began by performing a PCA on our existing dataset to reduce the dimensionality of the data. Of our original dataset, we retained only eight of the features. \n\nNext, we began exploring additional avenues in terms of features. Engineered features include time-based, graph-based, event-based, and miscellaneous features such as airport maintenance, airline reputations, and natural disasters"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"759020cd-62b5-495b-89ae-685548ddd6da","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### PCA on Dataset Features\n\nTo narrow down features in the original joined dataset, 3 PCAs were performed  on numeric columns. Below is a histogram of the PCA explained variances. Explained variance is a “statistical measure of how much variation in a dataset can be attributed to each of the principal components”. All of them are quite low, with a variance below 0.08. PC1 and PC2 have similar explained variances of 0.08 and 0.075 respectively while PC3 is at 0.05. This means that there is not much variation attributed to the principal components.\n\nIn heatmap of PCA scores, blue and red scores are the larger magnitude coefficients, and are more important in calculating the component. we have selected 17 variables with absolute value scores above 0.2. A correlation heatmap is plotted to remove intercorrelated features. Thus, our final features from the original dataset to 8 features:\n\n- 'dest_weather_Avg_HourlyDryBulbTemperature'\n- 'dest_weather_Avg_HourlyRelativeHumidity'\n- 'dest_weather_Avg_HourlyVisibility'\n- 'dest_weather_Sky_Conditions_OVC'\n- 'origin_weather_Avg_HourlyDryBulbTemperature'\n- 'origin_weather_Avg_HourlyRelativeHumidity'\n- 'origin_weather_Avg_HourlyVisibility'\n- 'origin_weather_Sky_Conditions_OVC'\n\nBelow are the functions we used to run PCA and display visualizations.\n\n\n<img src='https://drive.google.com/uc?id=1k-H9S2G5bBN7iwe_mgcxitFPNWBnPAO_' alt='Google Drive Image' width=25%/>\n<img src='https://drive.google.com/uc?id=1xLLalzsprkP5fVM7LeGpUiujPcgBEgug' alt='Google Drive Image' width=50%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"390548f5-f8c1-4410-812b-956d23ca9bea","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def run_PCA(df, cols):\n    new_df = df.select(cols)\n    df1 = new_df.na.drop(\"any\")\n    assembler = VectorAssembler(inputCols = cols, outputCol = 'features1')\n    df2 = assembler.transform(df1).select('features1')\n    #Standardize Features\n    scaler = StandardScaler(inputCol = 'features1',\n                           outputCol = 'scaledFeatures',\n                           withMean = True,\n                           withStd = True).fit(df2)\n    df_scaled = scaler.transform(df2)\n    #Fit Model\n    pca = PCA(k=3, inputCol='scaledFeatures', outputCol = 'pcaFeatures')\n    model2 = pca.fit(df_scaled)\n    components = model2.transform(df_scaled)\n    np.round(100.00*model2.explainedVariance.toArray(),3)\n    print(model2.explainedVariance)\n    # convert results to a dataframe\n    pcs = np.round(model2.pc.toArray(),3)\n    df_pc = pd.DataFrame(pcs, columns=['PC1', 'PC2', 'PC3'], index = cols)\n    df_pc.reset_index(inplace=True)\n    print(df_pc)\n    return df_pc, model2, components\n\ndef plot_pca(pca_model, threshold=0.2):\n    s = pca_model.explainedVariance\n    plt.bar(x = ['PC1', 'PC2', 'PC3'], height = s)\n    plt.title('PCA Explained Variance')\n    plt.show()\n    \n    # get correlation matrix plot for loadings\n    pca_df_reduced = pca_df_index.loc[(abs(pca_df_index['PC1']) > threshold) \\\n                                      | (abs(pca_df_index['PC2']) > threshold) \\\n                                      | (abs(pca_df_index['PC3']) > threshold),:]\n    fig, ax = plt.subplots(figsize=(10,20))      \n    sns.heatmap(pca_df, annot=True, cmap='Spectral', ax=ax)\n    fig.show()\n    \n    # PCA variables after removing correlated variables above 0.7. The variable with higher correlation with label will be selected.\n    \n    pc1_features = pca_df_index.loc[abs(pca_df_index['PC1']) > threshold, 'index'].values\n    pc2_features = pca_df_index.loc[abs(pca_df_index['PC2']) > threshold, 'index'].values\n    pc3_features = pca_df_index.loc[abs(pca_df_index['PC3']) > threshold, 'index'].values\n    selected_features = np.concatenate((pc1_features, pc2_features, pc3_features))\n    selected_features = list(set(selected_features))\n    corr = df_full.sample(0.01, seed=12).select(selected_features).toPandas().corr('pearson')\n    # build heatmap\n    f, ax = plt.subplots(figsize=(22, 20))\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, annot_kws={\"size\": 35 / np.sqrt(len(corr))})\n\n# read DF without new features\n# reduced_df = df.drop('CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'DEP_DELAY_NEW', 'DEP_DEL15', 'DEP_DELAY_GROUP', 'WHEELS_OFF', 'WHEELS_ON','CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'ARR_DELAY_NEW', 'ARR_DEL15', 'ARR_DELAY_GROUP', 'CANCELLED', 'ACTUAL_ELAPSED_TIME', 'FLIGHTS', 'DISTANCE_GROUP', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'YEAR', 'TOTAL_ADD_GTIME', 'FIRST_DEP_TIME', 'AIR_TIME',\n# 'origin_weather_Avg_HourlyStationPressure', 'origin_weather_Present_Weather_Mist',\n# 'dest_weather_Avg_HourlyStationPressure', 'dest_weather_Present_Weather_Mist'\n# 'OP_UNIQUE_CARRIER', 'OP_CARRIER', 'TAIL_NUM', 'ORIGIN', 'DEST', 'origin_airport_type', 'dest_airport_type', 'MAINT_DELAY_PCT_CAT', 'EVENT')\n# numeric_features = [f.name for f in reduced_df.schema.fields if isinstance(f.dataType, NumericType)]\n# pca_df, pca_model, components = run_PCA(df, numeric_features)\n# plot_pca(pca_model, threshold=0.2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c3503ec-5cb3-421d-981a-5bf6b6d0b7fa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Time-based Features: Time of Day"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c1441a07-3d91-471c-804a-b06dfab9b17c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["The first feature we implemented was a time-based feature focused on identifying flight patterns and when flights are flown most often. The flights are broken out by 4hour increments. For example, category one captures flights departing between 12:00AM - 4:00AM, category two is from 4:00AM - 8:00AM etc. We graphed this out in the chart below. This is further broken out to  a binary bar chart, separating delayed and non-delayed flights. The line in the chart below helps us determine the delayed flights as percentage of total flights in each category. \n\nAs we can see, delayed flights tend to spike up more during the late evening and night, as a prcentage of flown flights. We can interpret that as the airport potentially having fewer resources or perhaps work is not as efficient during those hours, resulting in a delay. This feature may prove to be useful when determining flight delay patterns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dee71e02-d5bc-4ebe-a512-3688f96406c9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<img src='https://drive.google.com/uc?id=1cYrv_N8lVZznIboMjtJATNSovcXk0Rek' alt='Google Drive Image' width=50%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fdc6df82-148e-423e-b764-d33f1b19b9e8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def add_timeofday(df):\n    def categorize(date_time):\n#         formatted_time = datetime.datetime.strptime(date_time, \"%Y-%m-%d %H:%M:%S\")\n#         hour = formatted_time.hour\n        hour = date_time.hour\n        if hour < 4:\n            category = 1\n        elif hour < 8:\n            category = 2\n        elif hour < 12:\n            category = 3\n        elif hour < 16:\n            category = 4\n        elif hour < 20:\n            category = 5\n        else:\n            category = 6\n        return category\n\n    categorize_udf = udf(categorize)\n    categorized_df = df.withColumn(\"time_of_day\", categorize_udf(df['_utc_dept_ts'])).withColumn(\"time_of_day\", col(\"time_of_day\").cast(\"string\"))\n        \n    return categorized_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4f68ad31-96fe-4db3-9224-040e9d9cbd76","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### National Events"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9dfc45d-c159-46b1-b636-45b4c6b7371c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Many passengers opt to fly around holidays for a variety of reasons. Some passengers take advantage of a longer weekend while others fly during these times due to special deals that may be available for flights. Regardless, the holidays are a busy time for all, particularly around Thanksgiving, Christmas, and New Years. This feature was created to classify dates around that period. Due to the uptick in flights around the busier times of year, delays may be expected to occur more frequently. \n\nIn the feature, we identify flights that are flown in a plus and minus seven day window to a holiday. We opted for a one week buffer because passengers tend to fly to their destinations around a week in advanced at times and when they depart, they may also follow a similar pattern.\n\nBelow is a diagram describing the flights around these holidays. The blue and green represent the number of non-delayed versus delayed flights respectively. The yellow line represents the percentage of delayed flights per holiday and the red line represents the percentage of delayed flights, on average, for non-holiday related flights. As we can see, certain holidays have a much higher delay percentage on average. That being said, the events make up only a fraction of the year as a whole."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"380e554e-06d6-4bc2-8d8c-d9f0d6eadd1d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<img src='https://drive.google.com/uc?id=1jib4zK963UsjPDwc4Cg0mFeWKguMFsVi' alt='Google Drive Image' width=100%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e7d9c5d2-981c-4a33-ad14-ca6e934c5fb9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def add_events(df):\n    from datetime import date\n    import holidays\n\n    us_holidays = holidays.US()\n    h_list = []\n    y = [2015, 2016, 2017, 2018, 2019, 2020, 2021]\n    for ptr in holidays.US(years = y).items():\n        check = '(Observed)'\n        if check not in ptr[1]:\n            h_list.append(ptr)\n\n    valid_days = []\n    for day in h_list:\n        for t in range(0,8):\n            valid_days.append((day[0] + timedelta(days=t), day[1]))\n            valid_days.append((day[0] - datetime.timedelta(days=t), day[1]))\n    valid_days = set(valid_days)\n    valid_days = list(valid_days)\n    d = [x[0] for x in valid_days]\n    h = [x[1] for x in valid_days]\n    holidates = pd.DataFrame({'DATES':d, 'EVENT': h})\n    c_holidates = holidates.groupby('DATES')['EVENT'].apply(lambda x: ','.join(x.astype(str))).reset_index()\n    c_events = spark.createDataFrame(c_holidates)\n    c_events = c_events.withColumn('DATES', col('DATES').cast('string'))\n    df.createOrReplaceTempView('sub')\n    c_events.createOrReplaceTempView('events')\n    sql = \"\"\"select\n        sub.*,\n        events.EVENT\n        from\n        sub\n        left join events on (sub.FL_DATE = events.DATES)\n    \"\"\"\n    write_blob = spark.sql(sql)\n    write_blob = write_blob.fillna('No Event', subset = ['EVENT'])\n    \n    return write_blob\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"0ff94828-4015-4783-accb-1b634a96c64c","inputWidgets":{},"title":"add_events(df): returns sql.dataframeb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Graph-based Pagerank Airport\n\nThe graph-based feature is the PageRank of airports using the inbound and outbound airports in our joined dataset. The pagerank algorithm computes a rank based on the airport’s inbound and outbound flights. Since airports with many connections are more likely to be delayed and will propagate to other airports, pagerank can help our model take this into account.\n\nThe correlation between pagerank and DEP_DEL15 (our label) was 0.034, which is not very high. However, our logistic regression show that pagerank has a relatively high feature importance and coefficient."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a3ac7fc-cbaf-4096-8097-f07be49973f2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def initGraph(dataRDD):\n    \"\"\"\n    Spark job to read in the raw data and initialize an \n    adjacency list representation with a record for each\n    node (including dangling nodes).\n\n    Returns: \n        graphRDD -  a pair RDD of (node_id , (score, edges))\n        \n    \"\"\"\n    graphRDD = None\n    \n    def combine(x, y):\n        # combine edges in reduce step\n        if x and y:\n            return x + \",\" + y\n        elif x and not y:\n            return x\n        else:\n            return y\n    graphRDD = dataRDD.map(tuple) \\\n                      .reduceByKey(combine).cache()\n    \n    # initialize score \n    N = sc.broadcast(graphRDD.count()).value\n    graphRDD = graphRDD.mapValues(lambda x: (1 / float(N), x))\n    \n    graphRDD.unpersist()\n    \n    return graphRDD\n# HW5 provided FloatAccumulator class\nfrom pyspark.accumulators import AccumulatorParam\n\nclass FloatAccumulatorParam(AccumulatorParam):\n    \"\"\"\n    Custom accumulator for use in page rank to keep track of various masses.\n    \n    IMPORTANT: accumulators should only be called inside actions to avoid duplication.\n    We stringly recommend you use the 'foreach' action in your implementation below.\n    \"\"\"\n    def zero(self, value):\n        return value\n    def addInPlace(self, val1, val2):\n        return val1 + val2\n# HW5 part d - job to run PageRank\ndef runPageRank(graphInitRDD, alpha = 0.15, maxIter = 10, verbose = True):\n    \"\"\"\n    Spark job to implement page rank\n    Args: \n        graphInitRDD  - pair RDD of (node_id , (score, edges))\n        alpha         - (float) teleportation factor\n        maxIter       - (int) stopping criteria (number of iterations)\n        verbose       - (bool) option to print logging info after each iteration\n    Returns:\n        steadyStateRDD - pair RDD of (node_id, pageRank)\n    \"\"\"\n    # teleportation:\n    a = sc.broadcast(alpha)\n    \n    # damping factor:\n    d = sc.broadcast(1-a.value)\n    \n    # initialize accumulators for dangling mass & total mass\n    mmAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n    totAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n    \n    def getAccum(line, mmAccum, totAccum):\n        # calculates dangling and total mass\n        node, (score, edges) = line\n        if not edges:\n            # add dangling mass\n            mmAccum.add(score)\n        # add total mass\n        totAccum.add(score)\n    \n    def distribute(line):\n        node, (score, edges) = line\n        # yield node and edges\n        yield (node, (0.0, edges))\n        if edges:\n            edges = edges.split(',')\n            num_edges = float(len(edges))\n            # yield node and distributed mass for each neighbor\n            for edge in edges:\n                yield (edge, (score / num_edges, ''))\n    \n    def combine(x, y):\n        # combine score\n        score = x[0] + y[0]\n        # take the record containing edges\n        if x[1]:\n            edges = x[1]\n        else: \n            edges = y[1]\n        return (score, edges)\n        \n    def computeRank(line):\n        score, edges = line\n        score = a.value * (1 / G) + d.value * ((m / G) + score)\n        return (score, edges)\n    \n    # get total mass in the beginning\n    G = sc.broadcast(graphInitRDD.count()).value\n    steadyStateRDD = graphInitRDD\n    for iteration in range(maxIter):\n        # count total mass and dnagling mass\n        # get dangling mass\n        steadyStateRDD.foreach(lambda x: getAccum(x, mmAccum, totAccum))\n        m = sc.broadcast(mmAccum.value).value\n        # execute page rank \n        steadyStateRDD = steadyStateRDD.flatMap(distribute)\\\n                                       .reduceByKey(combine)\\\n                                       .mapValues(computeRank)\\\n                                       .cache()\n        # if verbose, print ranks of current iteration\n        if verbose:\n            print(f\"...Step {iteration}:\")\n            print(totAccum.value, mmAccum.value)\n            print(*steadyStateRDD.mapValues(lambda x: x[0]).collect(), sep = '\\n')\n        # reset accumulators for next iteration\n        mmAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n        totAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n    # reformat to (node, rank) \n    steadyStateRDD = steadyStateRDD.mapValues(lambda x: x[0])\n    steadyStateRDD.unpersist()\n    \n    return steadyStateRDD\n\ndef add_pagerank(df):\n    # initialize RDD\n    years = [2014, 2015, 2016, 2017, 2018, 2019, 2020]\n    for year in years:\n        dfy = df.filter(col('YEAR') == year)\n        dfRDD = dfy.select('origin_airport_iata', 'dest_airport_iata').rdd\n        wikiGraphRDD = initGraph(dfRDD)\n        # create DF with IATA and corresponding pagerank\n        nIter = 21\n        full_results = runPageRank(wikiGraphRDD, alpha = 0.15, maxIter = nIter, verbose = False)\n        flights_pagerank = spark.createDataFrame(full_results, \"IATA: string, PageRank: float\").sort('PageRank', ascending=False)\n        join_year = str(int(year) + 1)\n        flights_pagerank = flights_pagerank.withColumn('YEAR_J', F.lit(join_year))\n        if year == years[0]:\n            temp = flights_pagerank\n        else:\n            temp = temp.union(flights_pagerank)\n    flights_pagerank = temp\n    # check that df doesn't already have pagerank\n    if 'PageRank' in df.columns:\n        df = df.drop('PageRank')\n    \n    # join pagerank column to df\n    df_pagerank = df.join(flights_pagerank, (df.origin_airport_iata == flights_pagerank.IATA) & (df.YEAR == flights_pagerank.YEAR_J), 'left').drop('IATA')\n    df_pagerank1 = df_pagerank.fillna(0.0, subset=['PageRank'])\n    df_pagerank2 = df_pagerank1.withColumn('PageRank1', F.col('PageRank').cast('float')).drop('PageRank', 'YEAR_J')\n    df_pagerank3 = df_pagerank2.withColumnRenamed('PageRank1', 'PageRank')\n#     df_pagerank.write.mode('overwrite').parquet(f\"{blob_url}/joined_df_new/\")\n    return df_pagerank3"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"1cf9e5ef-4eff-410a-ab08-03969a34046b","inputWidgets":{},"title":"add_pagerank(df): returns sql.dataframe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def pagerank_impact(df):\n    corr = df_pagerank.corr('DEP_DEL15', 'pagerank')\n    print(f\"The correlation between DEP_DELAY and PageRank is {corr}\")\n    return corr"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"925bbfb6-11d5-4bdf-b9fc-11abbaa9ce5f","inputWidgets":{},"title":"pagerank_impact"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Natural Disaster\n\nFlights can get delayed due to natural disasters in the origin or destination. Natural disasters can be detected when there are abnormal weather anomalies. Therefore, the column 'natural_disaster' is 1 when the origin or destination average hourly values are 3 standard deviations away from the mean. The standard deviation multiplier is determined by iterating the multiplier to achieve the highest correlation with the target variable."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d96c2cee-39ff-450f-a92c-9a01c3181979","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def add_natural_disaster(df, multiplier = 3.0):\n    \"\"\"\n    IN: sql.dataframe with weather data\n    OUT: sql.dataframe with a binary column, \"natural_disaster\",\n    \"\"\"\n    selected_columns = [\n    #  'origin_weather_Avg_HourlyAltimeterSetting', # removed - airplane elevation\n     'origin_weather_Avg_HourlyDewPointTemperature', \n     'origin_weather_Avg_HourlyDryBulbTemperature', \n     'origin_weather_Avg_HourlyPressureChange', \n     'origin_weather_Avg_HourlyRelativeHumidity', \n     'origin_weather_Avg_HourlySeaLevelPressure', \n     'origin_weather_Avg_HourlyStationPressure', \n     'origin_weather_Avg_HourlyVisibility', \n     'origin_weather_Avg_HourlyWetBulbTemperature',\n    #  'origin_weather_Avg_HourlyWindDirection', # removed - doesn't affect weather\n     'origin_weather_Avg_HourlyWindGustSpeed', \n     'origin_weather_Avg_HourlyWindSpeed', \n    #  'dest_weather_Avg_HourlyAltimeterSetting', # removed - airplane elevation\n     'dest_weather_Avg_HourlyDewPointTemperature', \n     'dest_weather_Avg_HourlyDryBulbTemperature', \n     'dest_weather_Avg_HourlyPressureChange', \n     'dest_weather_Avg_HourlyRelativeHumidity', \n     'dest_weather_Avg_HourlySeaLevelPressure', \n     'dest_weather_Avg_HourlyStationPressure', \n     'dest_weather_Avg_HourlyVisibility', \n     'dest_weather_Avg_HourlyWetBulbTemperature', \n    #  'dest_weather_Avg_HourlyWindDirection', # removed - doesn't affect weather\n     'dest_weather_Avg_HourlyWindGustSpeed', \n     'dest_weather_Avg_HourlyWindSpeed']\n    df_summary = df.select(selected_columns).summary().toPandas().set_index('summary').transpose().astype('float')\n\n    df_summary['lower'] = df_summary['mean'] - multiplier * df_summary['stddev'] \n    df_summary['upper'] = df_summary['mean'] + multiplier * df_summary['stddev'] \n\n    new = df.withColumn('natural_disaster', F.when((~F.col('origin_weather_Avg_HourlyDewPointTemperature') \\\n                                                             .between(df_summary.loc['origin_weather_Avg_HourlyDewPointTemperature', 'lower'], df_summary.loc['origin_weather_Avg_HourlyDewPointTemperature', 'upper'])) \\\n                                                    | (~F.col('origin_weather_Avg_HourlyDryBulbTemperature') \\\n                                                             .between(df_summary.loc['origin_weather_Avg_HourlyDryBulbTemperature', 'lower'], df_summary.loc['origin_weather_Avg_HourlyDryBulbTemperature', 'upper'])) \\\n                                                    | (~F.col('origin_weather_Avg_HourlyPressureChange') \\\n                                                             .between(df_summary.loc['origin_weather_Avg_HourlyPressureChange', 'lower'], df_summary.loc['origin_weather_Avg_HourlyPressureChange', 'upper'])) \\\n                                                    | (~F.col('origin_weather_Avg_HourlyRelativeHumidity') \\\n                                                             .between(df_summary.loc['origin_weather_Avg_HourlyRelativeHumidity', 'lower'], df_summary.loc['origin_weather_Avg_HourlyRelativeHumidity', 'upper'])) \\\n                                                    | (~F.col('origin_weather_Avg_HourlySeaLevelPressure') \\\n                                                             .between(df_summary.loc['origin_weather_Avg_HourlySeaLevelPressure', 'lower'], df_summary.loc['origin_weather_Avg_HourlySeaLevelPressure', 'upper'])) \\\n                                                    | (~F.col('origin_weather_Avg_HourlyStationPressure') \\\n                                                             .between(df_summary.loc['origin_weather_Avg_HourlyStationPressure', 'lower'], df_summary.loc['origin_weather_Avg_HourlyStationPressure', 'upper'])) \\\n                                                    | (~F.col('origin_weather_Avg_HourlyVisibility') \\\n                                                             .between(df_summary.loc['origin_weather_Avg_HourlyVisibility', 'lower'], df_summary.loc['origin_weather_Avg_HourlyVisibility', 'upper'])) \\\n                                                    | (~F.col('origin_weather_Avg_HourlyWetBulbTemperature') \\\n                                                             .between(df_summary.loc['origin_weather_Avg_HourlyWetBulbTemperature', 'lower'], df_summary.loc['origin_weather_Avg_HourlyWetBulbTemperature', 'upper'])) \\\n                                                    | (~F.col('origin_weather_Avg_HourlyWindGustSpeed') \\\n                                                             .between(df_summary.loc['origin_weather_Avg_HourlyWindGustSpeed', 'lower'], df_summary.loc['origin_weather_Avg_HourlyWindGustSpeed', 'upper'])) \\\n                                                    | (~F.col('origin_weather_Avg_HourlyWindSpeed') \\\n                                                             .between(df_summary.loc['origin_weather_Avg_HourlyWindSpeed', 'lower'], df_summary.loc['origin_weather_Avg_HourlyWindSpeed', 'upper'])) \\\n                                                    | (~F.col('dest_weather_Avg_HourlyDewPointTemperature') \\\n                                                             .between(df_summary.loc['dest_weather_Avg_HourlyDewPointTemperature', 'lower'], df_summary.loc['dest_weather_Avg_HourlyDewPointTemperature', 'upper'])) \\\n                                                    | (~F.col('dest_weather_Avg_HourlyDryBulbTemperature') \\\n                                                             .between(df_summary.loc['dest_weather_Avg_HourlyDryBulbTemperature', 'lower'], df_summary.loc['dest_weather_Avg_HourlyDryBulbTemperature', 'upper'])) \\\n                                                    | (~F.col('dest_weather_Avg_HourlyPressureChange') \\\n                                                             .between(df_summary.loc['dest_weather_Avg_HourlyPressureChange', 'lower'], df_summary.loc['dest_weather_Avg_HourlyPressureChange', 'upper'])) \\\n                                                    | (~F.col('dest_weather_Avg_HourlyRelativeHumidity') \\\n                                                             .between(df_summary.loc['dest_weather_Avg_HourlyRelativeHumidity', 'lower'], df_summary.loc['dest_weather_Avg_HourlyRelativeHumidity', 'upper'])) \\\n                                                    | (~F.col('dest_weather_Avg_HourlySeaLevelPressure') \\\n                                                             .between(df_summary.loc['dest_weather_Avg_HourlySeaLevelPressure', 'lower'], df_summary.loc['dest_weather_Avg_HourlySeaLevelPressure', 'upper'])) \\\n                                                    | (~F.col('dest_weather_Avg_HourlyStationPressure') \\\n                                                             .between(df_summary.loc['dest_weather_Avg_HourlyStationPressure', 'lower'], df_summary.loc['dest_weather_Avg_HourlyStationPressure', 'upper'])) \\\n                                                    | (~F.col('dest_weather_Avg_HourlyVisibility') \\\n                                                             .between(df_summary.loc['dest_weather_Avg_HourlyVisibility', 'lower'], df_summary.loc['dest_weather_Avg_HourlyVisibility', 'upper'])) \\\n                                                    | (~F.col('dest_weather_Avg_HourlyWetBulbTemperature') \\\n                                                             .between(df_summary.loc['dest_weather_Avg_HourlyWetBulbTemperature', 'lower'], df_summary.loc['dest_weather_Avg_HourlyWetBulbTemperature', 'upper'])) \\\n                                                    | (~F.col('dest_weather_Avg_HourlyWindGustSpeed') \\\n                                                             .between(df_summary.loc['dest_weather_Avg_HourlyWindGustSpeed', 'lower'], df_summary.loc['dest_weather_Avg_HourlyWindGustSpeed', 'upper'])) \\\n                                                    | (~F.col('dest_weather_Avg_HourlyWindSpeed') \\\n                                                             .between(df_summary.loc['dest_weather_Avg_HourlyWindSpeed', 'lower'], df_summary.loc['dest_weather_Avg_HourlyWindSpeed', 'upper'])),\n                                                    1).otherwise(0))\n    return new\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"828ab9db-5567-4ecd-ab04-93b5bc1ac76d","inputWidgets":{},"title":"add_natural_disaster(df, multiplier=3.0): returns sql.dataframe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def natural_disaster_impact(df, corr=True):\n    if corr:\n        corr = df.corr('DEP_DEL15', 'natural_disaster')\n        print(\"corr\", corr)\n    new_grouped = df.groupby('DEP_DEL15', 'natural_disaster').count().toPandas()\n    sns.barplot(data=new_grouped, x='natural_disaster', y='count', hue='DEP_DEL15')\n    return corr"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"eb0b8bf7-af08-46a2-9602-c7882993a30a","inputWidgets":{},"title":"Natural Disaster EDA"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src='https://drive.google.com/uc?id=1AWr77UVFZY9CzzGciWoJ5kQ5AK2NUkwy' alt='Google Drive Image' width=%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"06ac9126-6c1c-4e4a-91eb-eac100cf0998","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Airline Reputation\nThe airpline reputation feature was created using a rolling window of the previous year's airline reputation for having delayed flights. For instance, if the flight date was 11-30-2021, the reputation would be calculated from 11-30-2020 to 11-29-2021. If an airline has a trailing reputation for delayed flights, we may be able to assume the same for the following year."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"56998736-4198-4085-b4e6-9a9c64eb9a62","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# helper functions for calculating year-long interval\ndef calc_interval_first(d):\n    curr_year = datetime.datetime.strptime(d, \"%Y-%m-%d\")\n    first_day = (curr_year + datetime.timedelta(days=-365)).strftime(\"%Y-%m-%d\")\n    return first_day\n\ndef calc_interval_last(d):\n    curr_year = datetime.datetime.strptime(d, \"%Y-%m-%d\")\n    last_day = (curr_year + datetime.timedelta(days=-1)).strftime(\"%Y-%m-%d\")\n    return last_day\n\ndef add_reputation(df):\n    # read past dataframe\n    df_past = spark.read.parquet(f\"{blob_url}/df_past_14_21/\")\n\n    # count the number of flights per day per carrier and whether or not they were delayed\n    t = df_past.groupby('FL_DATE', 'OP_UNIQUE_CARRIER', 'DEP_DEL15').count().withColumnRenamed('count', 'DAILY_COUNT').sort('FL_DATE', 'OP_UNIQUE_CARRIER')\n\n    windowvaltot1 = (Window.partitionBy('OP_UNIQUE_CARRIER').orderBy('FL_DATE')\n                 .rangeBetween(Window.unboundedPreceding, 0))\n\n    df_w_cumsum1 = t.withColumn('CUM_SUM', F.sum('DAILY_COUNT').over(windowvaltot1))\n\n    df_w_cumsum2 = df_w_cumsum1.filter(col('DEP_DEL15') == 1).withColumn('DEL_SUM', F.sum('DAILY_COUNT').over(windowvaltot1)).drop(col('DEP_DEL15')).drop('DAILY_COUNT')\n\n    # add interval columns to main dataframe\n    calc_interval_first_udf = udf(calc_interval_first)\n    calc_interval_last_udf = udf(calc_interval_last)\n\n    t2 = df.withColumn('REP_INTERVAL_FIRST', calc_interval_first_udf('FL_DATE')).withColumn('REP_INTERVAL_LAST', calc_interval_last_udf('FL_DATE'))\n    t2.createOrReplaceTempView('t2')\n\n    # create sub-df for first day and last day of interval with cumulative counts\n    rep_cumsum_F = df_w_cumsum2.withColumnRenamed(\"FL_DATE\", \"F_DATE\").withColumnRenamed(\"OP_UNIQUE_CARRIER\", \"F_AIRLINE\").withColumnRenamed(\"CUM_SUM\", 'F_CUM_SUM').withColumnRenamed(\"DEL_SUM\", 'F_DEL_SUM')\n    rep_cumsum_F.createOrReplaceTempView('rep_cumsum_F')\n\n    rep_cumsum_L = df_w_cumsum2.withColumnRenamed(\"FL_DATE\", \"L_DATE\").withColumnRenamed(\"OP_UNIQUE_CARRIER\", \"L_AIRLINE\").withColumnRenamed(\"CUM_SUM\", 'L_CUM_SUM').withColumnRenamed(\"DEL_SUM\", 'L_DEL_SUM')\n    rep_cumsum_L.createOrReplaceTempView('rep_cumsum_L')\n\n    # join cumulative counts sub-dfs to main dataframe\n    sql = \"\"\"select\n        t2.*,\n        rep_cumsum_F.F_CUM_SUM,\n        rep_cumsum_F.F_DEL_SUM,\n        rep_cumsum_L.L_CUM_SUM,\n        rep_cumsum_L.L_DEL_SUM\n        from\n        t2\n        left join rep_cumsum_F on ((t2.REP_INTERVAL_FIRST = rep_cumsum_F.F_DATE) and (t2.OP_UNIQUE_CARRIER = rep_cumsum_F.F_AIRLINE))\n        left join rep_cumsum_L on ((t2.REP_INTERVAL_LAST = rep_cumsum_L.L_DATE) and (t2.OP_UNIQUE_CARRIER = rep_cumsum_L.L_AIRLINE))\n    \"\"\"\n    t3 = spark.sql(sql)\n    t3 = t3.fillna(0, subset = ['F_CUM_SUM', 'F_DEL_SUM', 'L_CUM_SUM', 'L_DEL_SUM'])\n\n    # create reputation column \n    diff_cols = udf(lambda arr: arr[1]-arr[0], IntegerType())\n    t4 = t3.withColumn('CUM_SUM', diff_cols(array('F_CUM_SUM', 'L_CUM_SUM'))).withColumn('DEL_SUM', diff_cols(array('F_DEL_SUM', 'L_DEL_SUM')))\n    t5 = t4.withColumn('REPUTATION', (F.col(\"DEL_SUM\")/F.col(\"CUM_SUM\")))\n    cols = ('REP_INTERVAL_FIRST', 'REP_INTERVAL_LAST', 'F_CUM_SUM', 'F_DEL_SUM', 'L_CUM_SUM', 'L_DEL_SUM', 'CUM_SUM', 'DEL_SUM')\n    df_rep = t5.fillna(0, subset = ['REPUTATION']).drop(*cols)\n    return df_rep"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"0bb07f7a-d27d-4c20-b38c-6658390a4331","inputWidgets":{},"title":"add_reputation(df): returns sql.dataframe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Airport Maintenance"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a72e66e-e48f-4dfc-bb8b-536ec1d10030","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["For Airport maintenance, we used a lagging indicator to determine the overall maintenance delays an airport typically faces. For example, if there were one hundred flights in 2015, and 25% of those flights were delayed due to maintenance, they would have a \"reputation\" for being delayed 25% of the time in 2016. The percentage or probability of being delayed due to maintenance will only be carried up to one year. E.g. 2018 will only influence 2019 and 2019 would only influence 2020."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f703ec6c-b131-4cc4-b30c-69897b711fc6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def add_maintenance(df):\n    \n    def next_year(year):\n        return int(year)+1\n\n    def maint_delay_pct(delays, del15):\n        if delays:\n            return delays/del15\n        else:\n            return 0\n\n    delay_pct_udf = udf(maint_delay_pct)\n    next_year_udf = udf(next_year)\n    \n    df_maint = df.withColumn('MAINTENANCE_DELAY', when(F.col('CARRIER_DELAY').isNull(), 0).when(F.col('CARRIER_DELAY')>=45, 1).otherwise(0))\n    df_maint1 = df_maint.withColumn('COUNT_ROWS', when(F.col('_utc_dept_ts').isNull(), 0).otherwise(1)) \n    df_maint_delay = df_maint1.withColumn('NEXT_YEAR', next_year_udf('YEAR'))\n    df_maint_delay1 = df_maint_delay.select('ORIGIN','NEXT_YEAR', 'DEP_DEL15', 'COUNT_ROWS','MAINTENANCE_DELAY').na.drop(subset=['DEP_DEL15']).withColumn('DEP_DEL15',df.DEP_DEL15.cast('integer')).groupBy('ORIGIN', 'NEXT_YEAR').sum().orderBy('ORIGIN','NEXT_YEAR')\n    df_maint_delay2 = df_maint_delay1.withColumn('maint_delay_pct', delay_pct_udf('sum(MAINTENANCE_DELAY)', 'sum(DEP_DEL15)')).select('ORIGIN','NEXT_YEAR', 'MAINT_DELAY_PCT')\n    df_maint_delay3 = df_maint_delay2.withColumnRenamed('ORIGIN', 'ORIGIN_MAINT')\n    df_maint_final = df.join(df_maint_delay3, (df.ORIGIN == df_maint_delay3.ORIGIN_MAINT) & (df.YEAR == df_maint_delay3.NEXT_YEAR), 'left').drop('NEXT_YEAR').drop('ORIGIN_MAINT') \\\n                .withColumn('MAINT_DELAY_PCT', F.col('MAINT_DELAY_PCT').cast('int'))\n\n    \n    return df_maint_final"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"f25c19af-0f69-4f1d-a14d-1fb39a544ec4","inputWidgets":{},"title":"add_maintenance(df): returns sql.dataframe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Combine all the features"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04fb210f-b5a2-4700-84e0-5f8f7acb9b39","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["In this step, we take the time to run and combine all of our data. This includes cleaning the dataset as well as adding the above mentioned features into the dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c7531f6e-5c92-470a-bd0c-352cd0fb306f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def clean_data(df):\n    df = df.withColumn('YEAR', F.col('YEAR').cast('int')) \\\n            .withColumn('ARR_DEL15', F.col('ARR_DEL15').cast('int')) \\\n            .withColumn('DEP_DEL15', F.col('DEP_DEL15').cast('int')) \\\n            .withColumn('DISTANCE', F.col('DISTANCE').cast('float')) \\\n            .withColumn('dest_airport_elevation', F.col('dest_airport_elevation').cast('float')) \\\n            .withColumn('origin_airport_elevation', F.col('origin_airport_elevation').cast('float')) \\\n            .withColumn('MAINT_DELAY_PCT', F.col('MAINT_DELAY_PCT').cast('int')) \\\n            .withColumn('REPUTATION', F.col('REPUTATION').cast('float'))  \\\n            .filter(F.col('YEAR') >= 2015) \n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b60549ec-27f4-4ffa-9a70-8d0f379b6cbb","inputWidgets":{},"title":"Clean Data"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def clean_2014():\n    # TO BE REFINED\n#     df_2014_new = (df_2014.withColumn('FL_DATE', convert_time_udf('FL_DATE'))).withColumn('FL_DATE', col('FL_DATE').cast(StringType()))\n#     df_2014_new.write.parquet(f\"{blob_url}/df_2014_new\")\n    pass\ndef add_2014(df):\n    df_2014 = spark.read.parquet(f\"{blob_url}/df_2014_new/\")\n    df_out = (df.unionByName(df_2014, allowMissingColumns=True))\n    return df_out"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8fa85086-f529-49f4-ac2f-a82e6a17c885","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["##### JIMI'S DATAFRAME ####\ndata_BASE_DIR = \"dbfs:/mnt/mids-w261-joined\"\nmids_w261_joined = spark.read.parquet(f\"{data_BASE_DIR}\")\ndf1 = mids_w261_joined.filter(col('CANCELLED') == 0.0)\ndf2 = add_natural_disaster(df1)\ndf3 = add_timeofday(df2)\ndf4 = add_2014(df3)\ndf5 = add_pagerank(df4)\ndf6 = add_reputation(df5)\ndf7 = add_maintenance(df6)\ndf8 = add_events(df7)\ndf9 = clean_data(df8)\ndf9.write.mode('overwrite').parquet(f\"{blob_url}/df_full_14_21\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a188a930-f1a6-4d13-acd8-6b63ee87dbe1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Final Features"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"22fdb9a5-c675-4736-8e02-445e1f4c1936","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["In our final dataset, we feature engineered six new features while reducing the original dataset down to eight features. The final feature set contains a total of 15 features, with 9 weather features, 1 graph-based feature, 2 time-based features, and 3 flight features."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2f0cf44a-d031-4358-92e0-7868a210bf87","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<img src='https://drive.google.com/uc?id=1psqEcKjPPzuzNrep7_8hThwTzuyZQe2w' alt='Google Drive Image' width=%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3ad70d5d-0ee6-462e-9594-bf2a2b5b145d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Data Pipeline Discussion"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe1542f3-8b61-4cf6-bc17-176be4b3b10e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##Pipeline Run Time\nOur pipeline run time was initially pretty slow at around a 10-15 minutes for each pipeline. This improved significantly with function implementation and caching, and now averaging a few minutes."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4d745add-6865-474d-a1d0-ac7ed081391b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##Leakage\n\nLeakage is when information should not be known to the model at the time of prediction is fed to the model. An example is providing the weather data that occurs at the same time of the flight. But we want to predict the delay 2 hours prior to allow the customer time to react to the delay. Thus, The model should only have access to the weather data 2 hours before the flight when making a prediction for that flight. To prevent leakage or violating cardinal sins of ML, we ensure that we split and set aside data for train, validation, and test. 2021 year was only used for blind test evaluation at the end of the process. We avoided EDA on 2021 as much as possible due to being a blind test set. It was never used during the training process. We looked for any dependencies. All features were offset by minimum 2 hours if not more to ensure no data leakage to the model."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3cc926d5-be95-41d3-abbe-a44811513dcc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df_full_14_21 = spark.read.parquet(f\"{blob_url}/df_full_14_21/\")\nomit_columns = ['_utc_dept_ts', '_utc_dept_minus2_ts', '_utc_dept_actual_ts', '_utc_arr_ts', '_utc_arr_actual_ts', '_dep_time_str', '_local_dept_ts', '_local_dept_minus2_ts', '_local_dept_actual_ts', '_local_at_src_airport_arr_ts', '_local_at_src_airport_arr_actual_ts', 'QUARTER', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'OP_CARRIER_AIRLINE_ID', 'OP_CARRIER', 'TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN_AIRPORT_ID', 'ORIGIN_AIRPORT_SEQ_ID', 'ORIGIN_CITY_MARKET_ID', 'ORIGIN_CITY_NAME', 'ORIGIN_STATE_ABR', 'ORIGIN_STATE_FIPS', 'ORIGIN_STATE_NM', 'ORIGIN_WAC', 'DEST_AIRPORT_ID', 'DEST_AIRPORT_SEQ_ID', 'DEST_CITY_MARKET_ID', 'DEST_CITY_NAME', 'DEST_STATE_ABR', 'DEST_STATE_FIPS', 'DEST_STATE_NM', 'DEST_WAC', 'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'DEP_DELAY_NEW', 'DEP_DELAY_GROUP', 'DEP_TIME_BLK', 'TAXI_OUT', 'WHEELS_OFF', 'WHEELS_ON', 'TAXI_IN', 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'ARR_DELAY_NEW', 'ARR_DEL15', 'ARR_DELAY_GROUP', 'ARR_TIME_BLK', 'CANCELLED', 'CANCELLATION_CODE', 'DIVERTED', 'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'FLIGHTS', 'DISTANCE_GROUP', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'FIRST_DEP_TIME', 'TOTAL_ADD_GTIME', 'LONGEST_ADD_GTIME', 'origin_airport_iata', 'origin_airport_tz', 'origin_airport_iso_country', 'origin_airport_iso_region', 'origin_airport_ws_station_id', 'dest_airport_iata', 'dest_airport_tz', 'dest_airport_iso_country', 'dest_airport_iso_region', 'dest_airport_ws_station_id', 'origin_weather_Station', 'origin_weather_Datehour', 'dest_weather_Station', 'dest_weather_Datehour']\ndf_model = df_full_14_21.drop(*omit_columns) \\\n            .dropna('any', subset =['DEP_DEL15']) \\\n            .withColumnRenamed('DEP_DEL15', 'label')\ndf_model.write.mode('overwrite').parquet(f\"{blob_url}/df_model\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dfad83b9-e976-464a-8ebb-8e45a08cddd0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder,StringIndexer\nfrom pyspark.sql.types import NumericType, StringType\nfrom pyspark.ml import Pipeline\n\ndef build_ohe_pipeline(df, features):\n    # find string features\n    string_features = [f.name for f in df.select(features).schema.fields if isinstance(f.dataType, StringType)]\n    string_features_indexed = []\n    if string_features:\n        print('Need to encode ', string_features)\n        string_features_indexed = [string_features[i] + '_CAT' for i in range(len(string_features))]\n        indexer = StringIndexer(inputCols = string_features, outputCols = string_features_indexed, handleInvalid='skip')\n        encoder = OneHotEncoder(inputCols = string_features_indexed, outputCols = [f'{x}_OneHot' for x in string_features])\n        pipeline = Pipeline(stages=[indexer, encoder])\n    else:\n        pipeline = Pipeline(stages=[])\n    return (pipeline, string_features, string_features_indexed)\n\ndef ohe_df():\n    df = spark.read.parquet(f\"{blob_url}/df_model/\")\n    # one hot encode if needed, new features due to one hot encoding columns\n    ohe_pipeline, string_features, string_features_indexed = build_ohe_pipeline(df, df.columns)\n    df_model_ohe = ohe_pipeline.fit(df).transform(df) \\\n                                .drop(*string_features) \\\n                                .drop(*string_features_indexed)\n    return df_model_ohe\ndf_model_ohe = ohe_df()\ndf_model_ohe.write.mode('overwrite').parquet(f\"{blob_url}/df_model_ohe\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"466b6b4b-fd96-4666-9f5d-8dfec94eef1f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Novel Direction - SMOTE\n\nWhen the team ran the initial baseline model, the train F1 score was 0.52 whereas the test score was 0.28. We believe the Logistic Regression model we built was fine and the features we selected were presumed to have some predictive power. For a while, we wondered how we could improve the model. We went back to the dataset to perform some additional EDA to see if there was anything we might have been missing. After some time, we came to the realization that the label classification was imbalanced! The number of non-delayed flights outnumbered the delayed flights by a ratio of nearly 5:1. \n\nTo handle the class imbalance, we decided to implement a SMOTE technique where we would create synthetic data. This would bring the two classes closer to a balance with a final ratio of 0.97."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"67c8da8d-d46a-4153-957a-0abb8bf8f689","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.read.parquet(f\"{blob_url}/df_model\").filter(F.col('YEAR') < 2021)\n\ndef apply_smote(df):\n    @F.udf(returnType=VectorUDT())\n    def subtract_vector_udf(arr):\n        # Must decorate func as udf to ensure that its callback form is the arg to df iterator construct\n        a = arr[0]\n        b = arr[1]\n        if isinstance(a, SparseVector):\n            a = a.toArray()\n        if isinstance(b, SparseVector):\n            b = b.toArray()\n        array_ = a - b\n        return random.uniform(0, 1) * Vectors.dense(array_)\n\n    @F.udf(returnType=VectorUDT())\n    def add_vector_udf(arr):\n        # Must decorate func as udf to ensure that its callback form is the arg to df iterator construct\n        a = arr[0]\n        b = arr[1]\n        if isinstance(a, SparseVector):\n            a = a.toArray()\n        if isinstance(b, SparseVector):\n            b = b.toArray()\n        array_ = a + b\n        return Vectors.dense(array_)\n\n    #for categorical columns, must take its stringIndexed form (smote should be after string indexing, default by frequency)\n\n    def pre_smote_df_process(df,num_cols,cat_cols,target_col,index_suffix=\"_index\"):\n        '''\n        string indexer and vector assembler\n        inputs:\n        * df: spark df, original\n        * num_cols: numerical cols to be assembled\n        * cat_cols: categorical cols to be stringindexed\n        * target_col: prediction target\n        * index_suffix: will be the suffix after string indexing\n        output:\n        * vectorized: spark df, after stringindex and vector assemble, ready for smote\n        '''\n        if(df.select(target_col).distinct().count() != 2):\n            raise ValueError(\"Target col must have exactly 2 classes\")\n\n        if target_col in num_cols:\n            num_cols.remove(target_col)\n\n        # only assembled numeric columns into features\n        assembler = VectorAssembler(inputCols = num_cols, outputCol = 'features')\n        # index the string cols, except possibly for the label col\n        assemble_stages = [StringIndexer(inputCol=column, outputCol=column+index_suffix).fit(df) for column in list(set(cat_cols)-set([target_col]))]\n        # add the stage of numerical vector assembler\n        assemble_stages.append(assembler)\n        pipeline = Pipeline(stages=assemble_stages)\n        pos_vectorized = pipeline.fit(df).transform(df)\n\n    #     # drop original num cols and cat cols\n    #     drop_cols = num_cols+cat_cols\n\n        keep_cols = [a for a in pos_vectorized.columns]\n\n        vectorized = pos_vectorized.select(*keep_cols).withColumn('label',pos_vectorized[target_col])\n        vectorized = pos_vectorized\n\n        return vectorized\n\n    def smote(vectorized_sdf, bucket = .001, multiplier = 3, k = 5):\n        '''\n        contains logic to perform smote oversampling, given a spark df with 2 classes\n        inputs:\n        * vectorized_sdf: cat cols are already stringindexed, num cols are assembled into 'features' vector\n          df target col should be 'label'\n        * smote_config: config obj containing smote parameters\n        output:\n        * oversampled_df: spark df after smote oversampling\n        '''\n        dataInput_min = vectorized_sdf[vectorized_sdf['label'] == 1]\n        dataInput_maj = vectorized_sdf[vectorized_sdf['label'] == 0]\n\n        # LSH, bucketed random projection\n        brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",seed=1234, bucketLength=bucket)\n        # smote only applies on existing minority instances    \n        model = brp.fit(dataInput_min)\n        model.transform(dataInput_min)\n\n        # here distance is calculated from brp's param inputCol\n        self_join_w_distance = model.approxSimilarityJoin(dataInput_min, dataInput_min, float(\"inf\"), distCol=\"EuclideanDistance\")\n\n        # remove self-comparison (distance 0)\n        self_join_w_distance = self_join_w_distance.filter(self_join_w_distance.EuclideanDistance > 0)\n\n        over_original_rows = Window.partitionBy(\"datasetA\").orderBy(\"EuclideanDistance\")\n\n        self_similarity_df = self_join_w_distance.withColumn(\"r_num\", F.row_number().over(over_original_rows))\n\n        self_similarity_df_selected = self_similarity_df.filter(self_similarity_df.r_num <= k)\n\n        over_original_rows_no_order = Window.partitionBy('datasetA')\n\n        # list to store batches of synthetic data\n        res = []\n\n        # two udf for vector add and subtract, subtraction include a random factor [0,1]\n\n        # retain original columns\n        original_cols = dataInput_min.columns\n\n        for i in range(multiplier):\n            print(\"generating batch %s of synthetic instances\"%i)\n            # logic to randomly select neighbour: pick the largest random number generated row as the neighbour\n            df_random_sel = self_similarity_df_selected.withColumn(\"rand\", F.rand()).withColumn('max_rand', F.max('rand').over(over_original_rows_no_order))\\\n                                .where(F.col('rand') == F.col('max_rand')).drop(*['max_rand','rand','r_num'])\n            # create synthetic feature numerical part\n            df_vec_diff = df_random_sel.select('*', subtract_vector_udf(F.array('datasetA.features', 'datasetB.features')).alias('vec_diff'))\n            df_vec_modified = df_vec_diff.select('*', add_vector_udf(F.array('datasetA.features', 'vec_diff')).alias('features'))\n\n            # for categorical cols, either pick original or the neighbour's cat values\n            for c in original_cols:\n                # randomly select neighbour or original data\n                col_sub = random.choice(['datasetA','datasetB'])\n                val = \"{0}.{1}\".format(col_sub,c)\n                if c != 'features':\n                    # do not unpack original numerical features\n                    df_vec_modified = df_vec_modified.withColumn(c,F.col(val))\n\n            # this df_vec_modified is the synthetic minority instances,\n            df_vec_modified = df_vec_modified.drop(*['datasetA','datasetB','vec_diff','EuclideanDistance'])\n\n            res.append(df_vec_modified)\n\n        dfunion = reduce(DataFrame.unionAll, res)\n        # union synthetic instances with original full (both minority and majority) df\n\n        oversampled_df = dfunion.union(vectorized_sdf.select(dfunion.columns))\n        return oversampled_df\n\n    # MLP MODEL FINAL FEATURES...\n    year = ['YEAR']\n    label = ['label']\n    num  =['dest_weather_Avg_HourlyDryBulbTemperature','dest_weather_Avg_HourlyRelativeHumidity','dest_weather_Avg_HourlyVisibility','dest_weather_Sky_Conditions_OVC', 'origin_weather_Avg_HourlyDryBulbTemperature','origin_weather_Avg_HourlyRelativeHumidity','origin_weather_Avg_HourlyVisibility','origin_weather_Sky_Conditions_OVC', \n    'natural_disaster',\n    'PageRank',\n    'REPUTATION',\n    'MAINT_DELAY_PCT'\n    ]\n\n    cat =['time_of_day', 'EVENT']\n\n    total = label + features + cat + year\n\n    # ... MLP Model final features end.\n\n    df = df[total].dropna()\n    vectorized_df = pre_smote_df_process(df, num, cat, 'label')\n    smoted = smote(vectorized_df, .001, 4, 5)\n    smoted = smoted.withColumnRenamed('features', 'features_smote')\n    return smoted"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"875e63b9-b9a0-4959-8eb1-0b1bbced78a5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Early Stop\nFor the data we use for early stopping, we used the F1 score, an ML metric, of the model evaluated on a validation set of data. Due to flunctuations between F1 scores during training, we persevere for 3 epochs/ iterations to ensure consistent convergence. Early stopping helps to prevent underfitting/ overfitting the model on our metric of interest in an optimal manner. With early stopping, we can allow the model to train on many more epochs without having to manually test different number of iterations and thereby save time. This method ensures the model has trained enough to not underfit by ensuring at least a certain number of consistent validation scores. Conversely, the model is not overfitting to the train set by setting too many iterations beyond the first sign of convergence.  \n\nMultilayer perceptron in pyspark does not have a way to implement early stop. We created a custom function iterating the fitting for every 3 iterations by feeding the previous model weights. We split and held validation set to evaluate as our metric for early stop after every 3 iterations. If there are 3 f1 validation scores within a set tolerance of each other, the model is considered converged. Cross validation was implemented, but for this cross validation we treated all the data as independent compared to our prior approach of an expanding window due to time series dependency. This is for feasiblity and to see comparison of the effect on f1 score compared to the expanding window cross validation."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8db5842-2ba3-4ee2-a825-33ab20827ba1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def MNN_e(trainDF, features, maxIter=[50], layers=[], blockSize=[128], stepSize=[0.03], solver=['l-bfgs'], initialWeights=[None], tol=[1e-6]):\n    \"\"\"\n    IN: pyspark.sql.DataFrame, list of features, MLP parameters\n    OUTPUT: CrossValidatorModel\n    \"\"\"\n\n    # build pipeline\n    pipeline = build_pipeline('mlp', features)\n    # find out number of features after encoding\n    stage1 = pipeline.getStages()[0].transform(trainDF)\n    stage2 = pipeline.getStages()[1].fit(stage1).transform(stage1)\n    first_layer = stage2.schema['features_scaled'].metadata['ml_attr']['num_attrs']\n    # default layers: input layer of size (features) and output layer of size 2 (classes)\n    layers = [first_layer] + layers + [2]\n    pipeline.getStages()[-1].setLayers(layers)\n    # print model architecture\n    mlp_str = f\"MLP-{layers[0]}input-\" + \"sigmoid-\".join([str(i) for i in layers[1:]]) + \"softmax\"\n    print(f\"MLP Architecture:\\n {mlp_str}\")\n    \n    pipeline.getStages()[-1].setMaxIter(maxIter[0])\n    weights = []\n    train_score = []\n    counter = 0\n    i = 0\n    trainDF_s, testDF_s = trainDF.randomSplit([0.8, 0.2], seed = 4)\n\n    evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label')\n    while counter < 2:\n        print(f\"{i+3}th iteration\")\n        if weights != []:\n            pipeline.getStages()[-1].setInitialWeights(weights)\n        model = pipeline.fit(trainDF_s)\n        predictions = model.transform(testDF_s).select(\"prediction\", \"label\")\n        new_score = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n\n        print(f\"\\tnew train (f1 validation) score: {new_score}\")\n        if len(train_score) < 3:\n\n            if len(train_score) > 1:\n                print(f\"\\tprevious train score: {train_score[-1]}\")\n                print(f\"\\tscore difference in last 2 iterations: {abs(train_score[-1] - new_score)}\")\n                train_score.append(new_score)\n                \n                if (abs(train_score[-1] - new_score) < 0.01):\n                    counter += 1\n                else: \n                    if counter > 0:\n                        counter -= 1\n\n            else:\n                train_score.append(new_score)\n\n        else:\n            print(f\"\\tprevious train score: {train_score[-1]}\")\n            print(f\"\\tscore difference in last 2 iterations: {abs(train_score[-1] - new_score)}\")\n            if abs(train_score[-1] - new_score) < 0.01:\n                counter += 1\n               \n            else:\n                if counter > 0:\n                    counter -= 1\n\n            train_score.pop(0)\n            train_score.append(new_score)\n        weights = model.stages[-1].weights\n        i += 3\n    print(f\"\\tno improvements in the last 3 iterations, returning model with new weights and score\")\n    return [i, model]\ndef ki_mlp_cv(df, features):\n    cv_metric = []\n    iterations = []\n    evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label')\n    datasplits = df.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], seed = 2018)\n    for num, split in enumerate(datasplits):\n        train, test = split.randomSplit([0.8, 0.2], seed = 4)\n        itera, cv_model = MNN_e(train, features, maxIter=[3], layers=[], stepSize=[0.3])\n        cv_predictions = cv_model.transform(test).select(\"prediction\", \"label\")\n        cv_result = evaluator.evaluate(cv_predictions, {evaluator.metricName: \"f1\"})\n        cv_metric.append(cv_result)\n        iterations.append(itera)\n        print(f'completed {num + 1} fold...')\n    print(f'F1 Scores: {cv_metric}')\n    print(f'Iterations for each fold: {iterations}')\n    print(f'Avg F1 Score:{round(sum(cv_metric)/ len(cv_metric), 5)}')\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"28b531c8-2284-49c8-90b6-91f06a0844e6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def early_stop():\n#trainDF_smote_full = spark.read.parquet(f\"{blob_url}/df_model_ohe\")\n    df = spark.read.parquet(f\"{blob_url}/df_model_ohe\")\n    trainDF_full = df.filter(F.col('YEAR') < 2021).cache()\n\n    year = ['YEAR']\n    label = ['label']\n    features = ['dest_weather_Avg_HourlyDryBulbTemperature','dest_weather_Avg_HourlyRelativeHumidity','dest_weather_Avg_HourlyVisibility','dest_weather_Sky_Conditions_OVC', 'origin_weather_Avg_HourlyDryBulbTemperature','origin_weather_Avg_HourlyRelativeHumidity','origin_weather_Avg_HourlyVisibility','origin_weather_Sky_Conditions_OVC', \n    'time_of_day_OneHot',\n    'natural_disaster',\n    'PageRank',\n    'REPUTATION',\n    'MAINT_DELAY_PCT',\n    'EVENT_OneHot'\n    ]\n    trainDF = get_train_test(trainDF_full, features, label, year, undersample=True).cache()\n\n    ki_mlp_cv(trainDF, features)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3c1de7ee-b231-416d-a70e-6b56775d51f1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["early_stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1cf7502-5daa-44ac-8afc-690c1e2bb994","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Dropping 93833 rows...35436418 remaining.\nUndersampling trainDF...\nmajor to minor class ratio: 4.872839184945185\nComplete.\nCompleted pipeline set up.\nMLP Architecture:\n MLP-33input-2softmax\n3th iteration\n\tnew train (f1 validation) score: 0.6146004629006315\n6th iteration\n\tnew train (f1 validation) score: 0.6184627799140598\n9th iteration\n\tnew train (f1 validation) score: 0.6187965964792662\n\tprevious train score: 0.6184627799140598\n\tscore difference in last 2 iterations: 0.0003338165652063685\n12th iteration\n\tnew train (f1 validation) score: 0.6186520451573732\n\tprevious train score: 0.6187965964792662\n\tscore difference in last 2 iterations: 0.00014455132189306052\n\tno improvements in the last 3 iterations, returning model with new weights and score\ncompleted 1 fold...\nCompleted pipeline set up.\nMLP Architecture:\n MLP-33input-2softmax\n3th iteration\n\tnew train (f1 validation) score: 0.6152251238815939\n6th iteration\n\tnew train (f1 validation) score: 0.6194489252037869\n9th iteration\n\tnew train (f1 validation) score: 0.6196885744522864\n\tprevious train score: 0.6194489252037869\n\tscore difference in last 2 iterations: 0.0002396492484995072\n12th iteration\n\tnew train (f1 validation) score: 0.6199875265651671\n\tprevious train score: 0.6196885744522864\n\tscore difference in last 2 iterations: 0.00029895211288066825\n\tno improvements in the last 3 iterations, returning model with new weights and score\ncompleted 2 fold...\nCompleted pipeline set up.\nMLP Architecture:\n MLP-33input-2softmax\n3th iteration\n\tnew train (f1 validation) score: 0.6151538817321436\n6th iteration\n\tnew train (f1 validation) score: 0.619537092628224\n9th iteration\n\tnew train (f1 validation) score: 0.619505006219599\n\tprevious train score: 0.619537092628224\n\tscore difference in last 2 iterations: 3.208640862495393e-05\n12th iteration\n\tnew train (f1 validation) score: 0.6200649177155071\n\tprevious train score: 0.619505006219599\n\tscore difference in last 2 iterations: 0.0005599114959080609\n\tno improvements in the last 3 iterations, returning model with new weights and score\ncompleted 3 fold...\nCompleted pipeline set up.\nMLP Architecture:\n MLP-33input-2softmax\n3th iteration\n\tnew train (f1 validation) score: 0.6149359815972362\n6th iteration\n\tnew train (f1 validation) score: 0.6187170054451434\n9th iteration\n\tnew train (f1 validation) score: 0.6192637901498349\n\tprevious train score: 0.6187170054451434\n\tscore difference in last 2 iterations: 0.0005467847046914942\n12th iteration\n\tnew train (f1 validation) score: 0.6194269185997314\n\tprevious train score: 0.6192637901498349\n\tscore difference in last 2 iterations: 0.0001631284498965213\n\tno improvements in the last 3 iterations, returning model with new weights and score\ncompleted 4 fold...\nCompleted pipeline set up.\nMLP Architecture:\n MLP-33input-2softmax\n3th iteration\n\tnew train (f1 validation) score: 0.6157298831473197\n6th iteration\n\tnew train (f1 validation) score: 0.6192938469760493\n9th iteration\n\tnew train (f1 validation) score: 0.6195409902736961\n\tprevious train score: 0.6192938469760493\n\tscore difference in last 2 iterations: 0.00024714329764685683\n12th iteration\n\tnew train (f1 validation) score: 0.6197062909548696\n\tprevious train score: 0.6195409902736961\n\tscore difference in last 2 iterations: 0.00016530068117348584\n\tno improvements in the last 3 iterations, returning model with new weights and score\ncompleted 5 fold...\nF1 Scores: [0.6186998769495127, 0.619829147099881, 0.618770770307085, 0.6202555856174405, 0.6195783896768281]\nIterations for each fold: [12, 12, 12, 12, 12]\nAvg F1 Score:0.61943\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Dropping 93833 rows...35436418 remaining.\nUndersampling trainDF...\nmajor to minor class ratio: 4.872839184945185\nComplete.\nCompleted pipeline set up.\nMLP Architecture:\n MLP-33input-2softmax\n3th iteration\n\tnew train (f1 validation) score: 0.6146004629006315\n6th iteration\n\tnew train (f1 validation) score: 0.6184627799140598\n9th iteration\n\tnew train (f1 validation) score: 0.6187965964792662\n\tprevious train score: 0.6184627799140598\n\tscore difference in last 2 iterations: 0.0003338165652063685\n12th iteration\n\tnew train (f1 validation) score: 0.6186520451573732\n\tprevious train score: 0.6187965964792662\n\tscore difference in last 2 iterations: 0.00014455132189306052\n\tno improvements in the last 3 iterations, returning model with new weights and score\ncompleted 1 fold...\nCompleted pipeline set up.\nMLP Architecture:\n MLP-33input-2softmax\n3th iteration\n\tnew train (f1 validation) score: 0.6152251238815939\n6th iteration\n\tnew train (f1 validation) score: 0.6194489252037869\n9th iteration\n\tnew train (f1 validation) score: 0.6196885744522864\n\tprevious train score: 0.6194489252037869\n\tscore difference in last 2 iterations: 0.0002396492484995072\n12th iteration\n\tnew train (f1 validation) score: 0.6199875265651671\n\tprevious train score: 0.6196885744522864\n\tscore difference in last 2 iterations: 0.00029895211288066825\n\tno improvements in the last 3 iterations, returning model with new weights and score\ncompleted 2 fold...\nCompleted pipeline set up.\nMLP Architecture:\n MLP-33input-2softmax\n3th iteration\n\tnew train (f1 validation) score: 0.6151538817321436\n6th iteration\n\tnew train (f1 validation) score: 0.619537092628224\n9th iteration\n\tnew train (f1 validation) score: 0.619505006219599\n\tprevious train score: 0.619537092628224\n\tscore difference in last 2 iterations: 3.208640862495393e-05\n12th iteration\n\tnew train (f1 validation) score: 0.6200649177155071\n\tprevious train score: 0.619505006219599\n\tscore difference in last 2 iterations: 0.0005599114959080609\n\tno improvements in the last 3 iterations, returning model with new weights and score\ncompleted 3 fold...\nCompleted pipeline set up.\nMLP Architecture:\n MLP-33input-2softmax\n3th iteration\n\tnew train (f1 validation) score: 0.6149359815972362\n6th iteration\n\tnew train (f1 validation) score: 0.6187170054451434\n9th iteration\n\tnew train (f1 validation) score: 0.6192637901498349\n\tprevious train score: 0.6187170054451434\n\tscore difference in last 2 iterations: 0.0005467847046914942\n12th iteration\n\tnew train (f1 validation) score: 0.6194269185997314\n\tprevious train score: 0.6192637901498349\n\tscore difference in last 2 iterations: 0.0001631284498965213\n\tno improvements in the last 3 iterations, returning model with new weights and score\ncompleted 4 fold...\nCompleted pipeline set up.\nMLP Architecture:\n MLP-33input-2softmax\n3th iteration\n\tnew train (f1 validation) score: 0.6157298831473197\n6th iteration\n\tnew train (f1 validation) score: 0.6192938469760493\n9th iteration\n\tnew train (f1 validation) score: 0.6195409902736961\n\tprevious train score: 0.6192938469760493\n\tscore difference in last 2 iterations: 0.00024714329764685683\n12th iteration\n\tnew train (f1 validation) score: 0.6197062909548696\n\tprevious train score: 0.6195409902736961\n\tscore difference in last 2 iterations: 0.00016530068117348584\n\tno improvements in the last 3 iterations, returning model with new weights and score\ncompleted 5 fold...\nF1 Scores: [0.6186998769495127, 0.619829147099881, 0.618770770307085, 0.6202555856174405, 0.6195783896768281]\nIterations for each fold: [12, 12, 12, 12, 12]\nAvg F1 Score:0.61943\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Algorithms\n\nIn this project, we tried several different models. We tried logistic regression, decision tree, and multilayer perceptron classifier. We used logistic regression as our baseline since it was our most simplistic model. Each were fed the same dataset after vectorizing, standardizing, and one-hot encoding. We ran experiments testing each model with same baseline features that were filtered based on impact from the PCA results. Afterwards, we compared each on the test and training scores to determine the best model classifier on the baseline dataset. Next using the best model classifier we identified, we experimented with different combination of newly engineered features to determine impact of each feature. Further experiments were done with grid search to determine the optimal hyper parameters."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48d2ea59-7fbb-4df8-b322-1e627800bb8e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Preprocessing\n\nIn the processing step, we will select the feature we want to run on our model. Since the features we used only had about 3000 null values our of  and drop any rows with null values. The training dataset consists of data from 2015 to 2020 (inclusive) and the test dataset consists of data in 2021. To address the severe class imbalance in the training dataset, the majority class (not delayed flights) is undersampled down to about 20% of the data, which is equivalent to the ratio between major and minor class. Although most of the experiments are run using this preprocessing pipeline, we will also test the SMOTE strategy, which we will discuss in later stages. \n\n1. Select Features\n2. Drop NA's\n3. Sampling on TrainDF"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8464677c-2dd5-461f-8bfa-d34d508bec15","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def get_train_test(df_full, features, label, year, undersample=False):\n    \"\"\" Split train and test data as before and after 2021. \"\"\"\n    df_full = df_full.select(features + label + year)\n    trainDF = df_full.dropna('any')\n    num_rows = trainDF.count()\n    print(f\"Dropping {df_full.count() - num_rows} rows...{num_rows} remaining.\")\n    if undersample:\n        print(\"Undersampling trainDF...\")\n        major_df = trainDF.filter(F.col(label[0]) == 0)\n        minor_df = trainDF.filter(F.col(label[0]) == 1)\n        ratio = major_df.count()/minor_df.count()\n        print(f\"major to minor class ratio: {ratio}\")\n        # undersample the majority rows\n        sampled_majority_df = major_df.sample(False, 1/ratio)\n        trainDF = sampled_majority_df.unionAll(minor_df)\n    print(\"Complete.\")\n    return trainDF"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"07025ff0-137b-4f84-87a5-2f5cdd17ca9f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Modeling Pipeline Stages\n\nOur pipeline model consists of a vector assembler, standard scaler, and MLlib model. This pipeline allows us to vectorize and normalize our features before fitting to our model of choice.\n\n1. Vector Assembler\n2. Standard Scaler\n3. MLlib Model (Logistic Regression, Decision Tree, Multilayer Perceptron Classifier)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db1679e8-1651-4ef9-80bd-ca9f76c3b3b2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def build_pipeline(modelType, features):\n    # Vectorize\n    assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n    # Normalize data\n    scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withMean=True)\n    if modelType == 'lr':\n        model = LogisticRegression(labelCol=\"label\", featuresCol=\"features_scaled\")\n        pipeline = Pipeline(stages=[assembler, scaler, model])\n    elif modelType == 'mlp':\n        model = MultilayerPerceptronClassifier(labelCol='label', featuresCol='features_scaled')\n        pipeline = Pipeline(stages=[assembler, scaler, model])\n    elif modelType == 'dt':\n        # Decision tree does not need to scale data\n        model = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n        pipeline = Pipeline(stages=[assembler, model])\n    else:\n        print(\"Please specify modelType as lr, mlp, and dt\")\n    print('Completed pipeline set up.')\n    return pipeline\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2149753-70c2-4b60-9c7c-8e5c261ba51b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Custom Cross Validator\n\nFlight and weather data is a time series data, so we have implemented 5-fold expanding window cross validation by year. In the first fold, 2015 data is the train set and 2016 is the test set. In the second fold, 2015 and 2016 data were the train set and 2017 data is the test set. We continue this train/test split until we reach our last year of 2021 as the test set, and the years 2015-2020 as the train set.\n\n<img src=\"https://i.stack.imgur.com/fXZ6k.png\" alt=\"fXZ6k.png\" width=50%>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"786d60ac-97e3-413b-a414-a5a6db2da1a4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def custom_d(trainDF):\n    d = {}\n    i = 1\n    for year in range(2015, 2020):\n        key = 'df' + str(i)\n        d[key] = trainDF.filter(trainDF.YEAR <= year + 1)\\\n                       .withColumn('cv', F.when(trainDF.YEAR <= year, 'train')\n                                             .otherwise('test'))\n        i += 1\n    return d"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98588d1e-7884-469c-81e4-71f3b46f804e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# MLlib Models\n\nOur prediction model aims to predict whether or not a flight will be delayed for more than 15 minutes, 2 hours in advance. Our target variable is 'DEP_DEL15' from the flights dataset, which is a binary variable with 1 for more than 15 minutes delay and 0 for less than 15 minutes delay. To solve this binary classification problem, we will be using Logistic Regression as our baseline and Multilayer Perceptron Classifier to push performance. All of our models follow the same pre-processing and pipeline model function, with minimal adjustments to function calls according to object type.\n\n## Logistic Regression\n\nOur baseline model is logistic regression. It is a linear method with the logistic loss function: \n\n\\\\[ L(w;x,y) := \\log(1+\\exp(-yw^Tx)) \\\\]\n  \nwhere \\\\(w\\\\) is the weight matrix, and \\\\(x\\\\) is a new datapoint. The model makes a prediction by applying the logistic function:\n\\\\[\\mathrm{f}(z) = \\frac{1}{1 + e^{-z}}\\\\]\n\nwhere \\\\(z = w^T x\\\\).\n\nSource: [spark.apache.org](https://spark.apache.org/docs/3.3.1/ml-classification-regression.html)\n\n\n## Decision Tree\n\nIn Phase 3, 21 experiments were run in total, starting with the 17 baseline features produced from principal component analysis results. Then, graph-based and time-based features were added in a stepwise fashion to see their impact on the model. Since PCA only accounted for numerical columns, other categorical variables from the flights dataset (unique carrier, origin, and destination) were added one-by-one.\n\nThe flights categorical variables and time-of-day time-based feature did not improve the model. Our final model includes the 17 baseline weather features in addition to pagerank of airports, holiday_bool, and leading_flights_count. \n\nDecision trees is a popular method for classification and regression machine learning tasks because they were easy to interpret and can handle categorical features and non-linear feature interactions. It is a greedy algorithm that performs a recursive binary partitioning of the feature space. The splits are chosen to maximize the information gain at a tree node and node impurity (Gini impurity for our case) is a measure of homogeneity of the labels at the node.\n\n\\\\[\\text{IG}(D,s) = \\text{Impurity}(D) - \\frac{N_{left}}{N} \\text{Impurity}(D_{left}) - \\frac{N_{right}}{N} \\text{Impurity}(D_{right})\\\\]\n\n\\\\[ \\text{Gini}(D)= 1 - \\sum_{i=1}^{k} p{_i}^{2}\\\\]\n\nD is dataset that contains samples from k classes. Probability of samples belonging to class i at a given node denoted as pi\n\n\nSource: [spark.apache.org](https://spark.apache.org/docs/latest/mllib-decision-tree.html)\n\n## Multilayer Perceptron Classifier\n\nMultilayer perceptron classifier (MLPC) is a classifier based on the <a href=\"https://en.wikipedia.org/wiki/Feedforward_neural_network\">feedforward artificial neural network</a>.\nMLPC consists of multiple layers of nodes.\nEach layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes map inputs to outputs\nby a linear combination of the inputs with the node&#8217;s weights \\\\(w\\\\) and bias \\\\(b\\\\) and applying an activation function.\nThis can be written in matrix form for MLPC with \\\\(K+1\\\\) layers as follows:\n\\\\[\n\\mathrm{y}(x) = \\mathrm{f_K}(...\\mathrm{f_2}(w_2^T\\mathrm{f_1}(w_1^T x+b_1)+b_2)...+b_K)\n\\\\]\nNodes in intermediate layers use sigmoid (logistic) function:\n\\\\[\n\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n\\\\]\nNodes in the output layer use softmax function:\n\\\\[\n\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n\\\\]\nThe number of nodes \\\\(N\\\\) in the output layer corresponds to the number of classes.\n\nMLPC employs backpropagation for learning the model. We use the logistic loss function for optimization and L-BFGS as an optimization routine.\n\n\nSource: [spark.apache.org](https://spark.apache.org/docs/3.3.1/ml-classification-regression.html#multilayer-perceptron-classifier)\n\n\n### MLP Architecture 1\n\nOur first architecture for the MLP model has 8 features selected using PCA as the input, a single hidden layer with 4 neurons, and an output softmax layer with 2 neurons for the binary class. Most of the experiments are done using this architecture in order to benchmark and compare the effects of adjusting parameters and features.\n\n\n<img src='https://drive.google.com/uc?id=13-qJGyi-ErRvHmx06QRhN_K9CnM0SQC0' alt='Google Drive Image' width=50%/>\n\n\n### MLP Architecture 2\n\nOur first architecture for the MLP model has 8 features selected using PCA as the input, a two hidden layer with 4 neurons, and an output softmax layer with 2 neurons for the binary class. \n\n\n\n<img src='https://drive.google.com/uc?id=1465MWbc07hic9MzvtCpOmGWF-hRA5SV-' alt='Google Drive Image' width=50%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af321f4d-9d40-4da5-890d-596a46fb89d3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Grid Search Implementation\n\nFor MNN, we customized the grid search with parameters for maxIter, stepSize, and solver. We experimented step sizes 0.1 and 0.03 (default). Since step size of 0.03 produced better results by only a small margin (1e-4 for scores) at the cost of more time, we opted for 0.1 step sizes for quick experiments. We also tried different solvers, and found that l-bfgs produced the best result. Moreover, we looked at increasing the number of iterations, but since the results stayed the same, we kept it at 20 iterations for most trials. Lastly, we looked at different number of neurons and layers and will discuss further observations in the results."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a120f0a9-3524-4a9b-ae84-0b3dacedcc8c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def LR(trainDF, features, maxIter=[15], regParam=[0.0], elasticNetParam=[0.0]):\n    \"\"\"\n    IN: pyspark.sql.DataFrame, list of features, LR parameters\n    OUTPUT: CrossValidatorModel\n    \"\"\"\n    d = custom_d(trainDF)\n    # build pipeline\n    pipeline = build_pipeline('lr', features)\n    # set up grid search: estimator, set of params, and evaluator\n    grid = ParamGridBuilder() \\\n                .addGrid(pipeline.getStages()[-1].regParam, regParam) \\\n                .addGrid(pipeline.getStages()[-1].elasticNetParam, elasticNetParam) \\\n                .addGrid(pipeline.getStages()[-1].maxIter, maxIter) \\\n                .build()\n    # Evaluator sing F1 score for evaluator\n    evaluator = MulticlassClassificationEvaluator(metricName='fMeasureByLabel', beta=1)\n    # run cross validation & return the crossvalidation F0.5 score for 'test' set\n    cv = CustomCrossValidator(estimator=pipeline, estimatorParamMaps=grid, evaluator=evaluator,\n         splitWord = ('train', 'test'), cvCol = 'cv', parallelism=7)\n    start = time.time()\n    cvModel = cv.fit(d)  \n    print(f'... completed training in {time.time() - start} seconds.')\n    return (cvModel)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"03c1fa30-8ee4-421d-a2de-fd009805a870","inputWidgets":{},"title":"Logistic Regression"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def DT(trainDF, features):\n    \"\"\"\n    IN: pyspark.sql.DataFrame, list of features, LR parameters\n    OUTPUT: CrossValidatorModel\n    \"\"\"\n    d = custom_d(trainDF)\n    # build pipeline\n    pipeline = build_pipeline('dt', features)\n    # set up grid search: estimator, set of params, and evaluator\n    grid = ParamGridBuilder() \\\n                .build()\n    # Evaluator sing F1 score for evaluator\n    evaluator = MulticlassClassificationEvaluator(metricName='fMeasureByLabel', beta=1)\n    # run cross validation & return the crossvalidation F0.5 score for 'test' set\n    cv = CustomCrossValidator(estimator=pipeline, estimatorParamMaps=grid, evaluator=evaluator,\n         splitWord = ('train', 'test'), cvCol = 'cv', parallelism=7)\n    start = time.time()\n    cvModel = cv.fit(d)  \n    print(f'... completed training in {time.time() - start} seconds.')\n    return (cvModel)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"5f48e4e5-0ba1-48fd-99ce-ebc99b5a631d","inputWidgets":{},"title":"Decision Tree"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def MNN(trainDF, features, maxIter=[50], layers=[], blockSize=[128], stepSize=[0.03], solver=['l-bfgs'], initialWeights=[None], tol=[1e-6]):\n    \"\"\"\n    IN: pyspark.sql.DataFrame, list of features, MLP parameters\n    OUTPUT: CrossValidatorModel\n    \"\"\"\n    d = custom_d(trainDF)\n    # build pipeline\n    pipeline = build_pipeline('mlp', features)\n    # find out number of features after encoding\n    stage1 = pipeline.getStages()[0].transform(trainDF)\n    stage2 = pipeline.getStages()[1].fit(stage1).transform(stage1)\n    first_layer = stage2.schema['features_scaled'].metadata['ml_attr']['num_attrs']\n    # default layers: input layer of size (features) and output layer of size 2 (classes)\n    layers = [first_layer] + layers + [2]\n    pipeline.getStages()[-1].setLayers(layers)\n    # print model architecture\n    mlp_str = f\"MLP-{layers[0]}input-\" + \"sigmoid-\".join([str(i) for i in layers[1:]]) + \"softmax\"\n    print(f\"MLP Architecture:\\n {mlp_str}\")\n    # set up grid search: estimator, set of params, and evaluator\n    grid = ParamGridBuilder() \\\n                .addGrid(pipeline.getStages()[-1].maxIter, maxIter) \\\n                .addGrid(pipeline.getStages()[-1].blockSize, blockSize) \\\n                .addGrid(pipeline.getStages()[-1].stepSize, stepSize) \\\n                .addGrid(pipeline.getStages()[-1].solver, solver) \\\n                .addGrid(pipeline.getStages()[-1].tol, tol) \\\n                .build()\n\n    # Evaluator sing F1 score for evaluator\n    evaluator = MulticlassClassificationEvaluator(metricName='fMeasureByLabel', beta=1)\n    # run cross validation & return the crossvalidation F0.5 score for 'test' set\n    cv = CustomCrossValidator(estimator=pipeline, estimatorParamMaps=grid, evaluator=evaluator,\n         splitWord = ('train', 'test'), cvCol = 'cv', parallelism=7)\n    start = time.time()\n    cvModel = cv.fit(d)  \n    print(f'... completed training in {time.time() - start} seconds.')\n    return (cvModel)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"5a863346-5771-44dd-897d-c022ea9e07f1","inputWidgets":{},"title":"Multilayer Neural Network"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Evaluation Metrics\n\nF1 score was our method of choice due to flight prediction delay set being inherently imbalanced with many more non-delayed flights compared to delay flights. As a harmonic mean of precision and recall, F1 score helps us detect a situation where the model predicts a singular value for every situation due to uneven distribution of classes. F1 score is also a singular number with simple interpretation that enables ease of comparison. We weighed precision and recall equally due to our concern of overpredicting only a single class for all values. We believe customers value being able to reliably know whether their flight is delayed ahead of time increases customer satisfaction and thereby retain their business. In this business case, we hope to use customer life time value and customer retention percentage to monitor the performance of our model in a business setting."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5ac95cdb-1fe1-495a-9fa6-0dcb13365873","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Model performance is derived from the matrix of true positive (TP), true negative (TN), false positive (FP), and false negative (FN). Metrics are derived from this matrix and helps to place importance on one or more of these counts.\n\n\n<img decoding=\"async\" src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Confusion-Matrix-balanced-accuracy.png?resize=552%2C371&#038;ssl=1\" alt=\"Confusion Matrix - balanced accuracy\" class=\"wp-image-51169\" width=40% data-recalc-dims=\"1\">\n\n\nBoth F1 score and balanced accuracy needs to be considered because F1 score doesn't care about how many true negatives are being classified while balanced accuracy does not consider true positives. \n\n**F1-score** balances precision and recall. \n$$ \\text{F1} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\nIn F1, **Recall**, also known as sensitivity, is the number of true positives out of true positives and false negatives \\\\((\\text{Recall}=\\text{TP}/(\\text{TP}+\\text{FN}))\\\\). **Precision** is the number of true positives out of all the true positives and false positives \\\\((\\text{Precision}=\\text{TP}/(\\text{TP}+\\text{FP}))\\\\). \n\nOn the other hand, **Balanced accuracy** is a mean of recall and specificity.\n$$ \\text{Balanced Accuracy} = \\frac{\\text{Recall} + \\text{specificity}}{2} $$\nIn balanced accuracy, specificity is the true negative rate \\\\((\\text{Specificity}=\\text{TN}/(\\text{TN}+\\text{FP}))\\\\).\n\n\nSource: [neptune.ai](https://neptune.ai/blog/balanced-accuracy#:~:text=F1%2Dscore,accuracy%20of%20an%20individual%20test.)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5a1aeb8-95d5-4a1e-9e5e-ac6abd5f9362","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def training_summary_mlp(pipeline_model, params=False):\n    # Model Architecture\n    layers = pipeline_model.stages[-1].getLayers()\n    print(\"MLP Model Architecture: \")\n    mlp_str = f\"MLP-{layers[0]}input-\" + \"sigmoid-\".join([str(i) for i in layers[1:]]) + \"softmax\"\n    print(mlp_str)\n    if params:\n        # model parameters\n        print(\"MLP Model Parameters: \")\n        pprint(pipeline_model.stages[-1].extractParamMap())\n    # Training Metrics\n    training_summary = pipeline_model.stages[-1].summary()\n    metrics = dict()\n    metrics['fMeasureByLabel'] = [training_summary.fMeasureByLabel()]\n    metrics['weightedFMeasure'] = [training_summary.weightedFMeasure()]\n    metrics['accuracy'] = [training_summary.accuracy]\n    metrics['weightedPrecision'] = [training_summary.weightedPrecision]\n    metrics['weightedRecall'] = [training_summary.weightedRecall]\n    metrics['weightedTruePositiveRate'] = [training_summary.weightedTruePositiveRate]\n    metrics['weightedFalsePositiveRate'] = [training_summary.weightedFalsePositiveRate]\n    metrics['truePositiveRateByLabel'] = [training_summary.truePositiveRateByLabel]\n    metrics['falsePositiveRateByLabel'] = [training_summary.falsePositiveRateByLabel]\n    metrics['precisionByLabel'] = [training_summary.precisionByLabel]\n    metrics['recallByLabel'] = [training_summary.recallByLabel]\n    metrics['totalIterations'] = [training_summary.totalIterations]\n    metrics['balancedAccuracy'] = (metrics['weightedRecall'] + 1 - metrics['weightedFalsePositiveRate']) / 2\n    pred_df = training_summary.predictions\n    pprint(metrics)\n    print('Complete.')\n    return (metrics, pred_df)\n\ndef evaluate_mlp(pipeline_model, testDF):\n    # Model Architecture\n    print(\"MLP Model Architecture: \")\n    layers = pipeline_model.stages[-1].getLayers()\n    mlp_str = f\"MLP-{layers[0]}input-\" + \"sigmoid-\".join([str(i) for i in layers[1:]]) + \"softmax\"\n    print(mlp_str)\n    # model parameters\n    print(\"MLP Model Parameters: \")\n    pprint(pipeline_model.extractParamMap())\n    # predict on testDF\n    pred_df = pipeline_model.transform(testDF)\n    pred_df.select('features_scaled','label','rawPrediction','probability','prediction').groupby('prediction').count().show()\n    # metrics\n    metricNames = [\n                 'f1',\n                 'accuracy',\n                 'weightedPrecision',\n                 'weightedRecall',\n                 'weightedTruePositiveRate',\n                 'weightedFalsePositiveRate',\n                 'weightedFMeasure',\n                 'truePositiveRateByLabel',\n                 'falsePositiveRateByLabel',\n                 'precisionByLabel',\n                 'recallByLabel',\n                 'fMeasureByLabel',\n                 'logLoss',\n                 ]\n    # test metrics\n    evaluator = MulticlassClassificationEvaluator(labelCol = 'label', predictionCol = 'prediction')\n    metrics = dict()\n    for metricName in metricNames:\n        evaluator.setMetricName(metricName)\n        metrics[metricName] = [evaluator.evaluate(pred_df)]\n        print(f\"{metricName}: {metrics[metricName]}\")\n    metrics['balancedAccuracy'] = (metrics['weightedRecall'] + 1 - metrics['weightedFalsePositiveRate']) / 2\n    print('Complete.')\n    # prediction dataframes\n    return (metrics, pred_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"edad5c0d-6245-4bad-8742-a9feca4c0ca3","inputWidgets":{},"title":"MLPC - evaluation functions"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def training_summary(pipeline_model, params=False):\n    training_summary = pipeline_model.stages[-1].summary\n\n    # Obtain the objective per iteration\n    objectiveHistory = training_summary.objectiveHistory\n    plt.plot(objectiveHistory)\n    plt.ylabel('Scaled loss + regularization')\n    plt.xlabel('Iteration')\n    plt.title('Objective History')\n    plt.show()\n\n    # Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n    roc = training_summary.roc.toPandas()\n    plt.plot(roc['FPR'], roc['TPR'])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.title('areaUnderROC')\n    plt.show()\n    print(\"areaUnderROC: \" + str(training_summary.areaUnderROC))\n\n\n    # Obtain the precision-recall curve\n    pr = training_summary.pr.toPandas()\n    plt.plot(pr['recall'], pr['precision'])\n    plt.ylabel('Precision')\n    plt.xlabel('Recall')\n    plt.title('Precision-Recall Curve')\n    plt.show()\n\n    # Set the model threshold to maximize F-Measure\n    fMeasure = training_summary.fMeasureByThreshold\n    maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n    bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n        .select('threshold').head()['threshold']\n    print(f\"bestThreshold: {bestThreshold}\")\n\n    regParam = cvModel1.bestModel.stages[-1].getRegParam()\n    elasticNetParam =  cvModel1.bestModel.stages[-1].getElasticNetParam()\n    lr_str = f\"LR-{regParam}regParam-{elasticNetParam}elasticNetParam\"\n    print(lr_str)\n    if params:\n        # model parameters\n        print(\"Model Parameters: \")\n        pprint(pipeline_model.stages[-1].extractParamMap())\n    # Training Metrics\n    metrics = dict()\n    metrics['fMeasureByLabel'] = [training_summary.fMeasureByLabel()]\n    metrics['weightedFMeasure'] = [training_summary.weightedFMeasure()]\n    metrics['accuracy'] = [training_summary.accuracy]\n    metrics['weightedPrecision'] = [training_summary.weightedPrecision]\n    metrics['weightedRecall'] = [training_summary.weightedRecall]\n    metrics['weightedTruePositiveRate'] = [training_summary.weightedTruePositiveRate]\n    metrics['weightedFalsePositiveRate'] = [training_summary.weightedFalsePositiveRate]\n    metrics['truePositiveRateByLabel'] = [training_summary.truePositiveRateByLabel]\n    metrics['falsePositiveRateByLabel'] = [training_summary.falsePositiveRateByLabel]\n    metrics['precisionByLabel'] = [training_summary.precisionByLabel]\n    metrics['recallByLabel'] = [training_summary.recallByLabel]\n    metrics['totalIterations'] = [training_summary.totalIterations]\n    metrics['balancedAccuracy'] = [float(metrics['weightedRecall'][0] + 1 - metrics['weightedFalsePositiveRate'][0]) / 2]\n\n    pred_df = training_summary.predictions\n    pprint(metrics)\n    print('Complete.')\n    return (metrics, pred_df)\n\ndef evaluate(pipeline_model, testDF):\n    regParam = cvModel1.bestModel.stages[-1].getRegParam()\n    elasticNetParam =  cvModel1.bestModel.stages[-1].getElasticNetParam()\n    lr_str = f\"LR-{regParam}regParam-{elasticNetParam}elasticNetParam\"\n    print(lr_str)\n    # model parameters\n    print(\"Model Parameters: \")\n    pprint(pipeline_model.extractParamMap())\n    # predict on testDF\n    pred_df = pipeline_model.transform(testDF)\n    pred_df.select('features_scaled','label','rawPrediction','probability','prediction').groupby('prediction').count().show()\n    # metrics\n    metricNames = [\n                 'f1',\n                 'accuracy',\n                 'weightedPrecision',\n                 'weightedRecall',\n                 'weightedTruePositiveRate',\n                 'weightedFalsePositiveRate',\n                 'weightedFMeasure',\n                 'truePositiveRateByLabel',\n                 'falsePositiveRateByLabel',\n                 'precisionByLabel',\n                 'recallByLabel',\n                 'fMeasureByLabel',\n                 'logLoss',\n                 ]\n    # test metrics\n    evaluator = MulticlassClassificationEvaluator(labelCol = 'label', predictionCol = 'prediction')\n    metrics = dict()\n    for metricName in metricNames:\n        evaluator.setMetricName(metricName)\n        metrics[metricName] = [evaluator.evaluate(pred_df)]\n        print(f\"{metricName}: {metrics[metricName]}\")\n\n    metricName = 'balancedAccuracy'\n    metrics['balancedAccuracy'] = [(metrics['weightedRecall'][0] + 1 - metrics['weightedFalsePositiveRate'][0]) / 2]\n    print(f\"{metricName}: {metrics[metricName]}\")\n    \n    print('Complete.')\n    # prediction dataframes\n    return (metrics, pred_df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"6003579e-edf9-4f27-abc3-3a5f8347b62a","inputWidgets":{},"title":"Training"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def evaluate_dt(pipeline_model, features, testDF, trainDF):\n    # about the model\n    model = pipeline_model.stages[-1]\n    print(f\"Feature Importance: \")\n    feat_dict= {}\n    for col, val in sorted(zip(features, model.featureImportances),key=lambda x:x[1],reverse=True):\n      feat_dict[col]=val\n    feat_df = pd.DataFrame({'Feature':feat_dict.keys(),'Importance':feat_dict.values()})\n    values = feat_df.Importance\n    idx = feat_df.Feature\n    plt.figure(figsize=(10,8))\n    clrs = ['green' if (x < values.max()) else 'red' for x in values ]\n    sns.barplot(y=idx,x=values,palette=clrs).set(title='Important features')\n    plt.show()\n\n    print(\"TRAIN DF Metrics\")\n    predictions = pipeline_model.transform(trainDF)\n    # compute TN, TP, FN, and FP\n    # Calculate the elements of the confusion matrix\n    TN = predictions.filter('prediction = 0 AND label = prediction').count()\n    TP = predictions.filter('prediction = 1 AND label = prediction').count()\n    FN = predictions.filter('prediction = 0 AND label <> prediction').count()\n    FP = predictions.filter('prediction = 1 AND label <> prediction').count()\n    # calculate accuracy, precision, recall, and F1-score\n    accuracy = (TN + TP) / (TN + TP + FN + FP)\n    precision = TP / (TP + FP)\n    recall = TP / (TP + FN)\n    f1 = 2 * precision * recall / (precision + recall)\n    balancedAccuracy = (precision + 1 - recall)/2\n    # print metrics \n    print('n precision: %0.3f' % precision)\n    print('n recall: %0.3f' % recall)\n    print('n accuracy: %0.3f' % accuracy)\n    print('n f1 score: %0.3f' % f1)\n    print('n balancedAccuracy: %0.3f' % balancedAccuracy)\n    bcEvaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n    # Evaluate the model's performance based on area under the ROC curve and accuracy \n    print(f\"Area under ROC curve: {bcEvaluator.evaluate(predictions)}\")\n    train_metrics = MulticlassClassificationEvaluator(metricName=\"weightedFMeasure\").evaluate(cvModel1.bestModel.transform(trainDF))\n    print(\"weightedFMeasure: \", train_metrics)\n    \n    print(\"TEST DF Metrics\")\n    predictions = pipeline_model.transform(testDF)\n    # compute TN, TP, FN, and FP\n    # Calculate the elements of the confusion matrix\n    TN = predictions.filter('prediction = 0 AND label = prediction').count()\n    TP = predictions.filter('prediction = 1 AND label = prediction').count()\n    FN = predictions.filter('prediction = 0 AND label <> prediction').count()\n    FP = predictions.filter('prediction = 1 AND label <> prediction').count()\n    # calculate accuracy, precision, recall, and F1-score\n    accuracy = (TN + TP) / (TN + TP + FN + FP)\n    precision = TP / (TP + FP)\n    recall = TP / (TP + FN)\n    f1 = 2 * precision * recall / (precision + recall)\n    balancedAccuracy = (precision + 1 - recall)/2\n    # print metrics \n    print('n precision: %0.3f' % precision)\n    print('n recall: %0.3f' % recall)\n    print('n accuracy: %0.3f' % accuracy)\n    print('n f1 score: %0.3f' % f1)\n    print('n balancedAccuracy: %0.3f' % balancedAccuracy)\n    bcEvaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n    # Evaluate the model's performance based on area under the ROC curve and accuracy \n    print(f\"Area under ROC curve: {bcEvaluator.evaluate(predictions)}\")\n    bcEvaluator = MulticlassClassificationEvaluator(metricName=\"weightedFMeasure\")\n    # Evaluate the model's performance based on area under the ROC curve and accuracy \n    print(f\"weightedFMeasure: {bcEvaluator.evaluate(test_metrics)}\")\n    \n    return predictions\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"207801b1-3687-4f4c-95c1-d223da344ef3","inputWidgets":{},"title":"Evaluate"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndef save_train_metrics(metrics, pred_df, savefolder, overwrite=False):\n    \" Save train metrics to blob \"\n    if f'/' in savefolder[-1]: \n        savefolder[:-1]\n    if not overwrite:\n        try:\n            spark.createDataFrame(pd.DataFrame.from_dict(metrics)).write.parquet(savefolder + '/train_metrics_df')\n            pred_df.write.parquet(savefolder + '/train_pred_df')\n        except:\n            print(\"Save Error: 'overwrite' set to False but file exists.\")\n            return\n    else:\n        spark.createDataFrame(pd.DataFrame.from_dict(metrics)).write.mode('overwrite').parquet(savefolder + '/train_metrics_df')\n        pred_df.write.mode('overwrite').parquet(savefolder + '/train_pred_df')\n        # print message\n        print(f\"Saved test metrics to {savefolder}/train_metrics_df\")\n        print(f\"Saved test predictions to {savefolder}/train_pred_df\")\n    return\n\ndef save_test_metrics(metrics, pred_df, savefolder, overwrite=False):\n    \" save test metrics to blob \"\n    if f'/' in savefolder[-1]: \n        savefolder[:-1]\n    if not overwrite:\n        try:\n            spark.createDataFrame(pd.DataFrame.from_dict(metrics)).write.parquet(savefolder + '/test_metrics_df')\n            pred_df.write.parquet(savefolder + '/test_pred_df')\n        except:\n            print(\"Save Error: 'overwrite' set to False but file exists.\")\n            return\n    else:\n        spark.createDataFrame(pd.DataFrame.from_dict(metrics)).write.mode('overwrite').parquet(savefolder + '/test_metrics_df')\n        pred_df.write.mode('overwrite').parquet(savefolder + '/test_pred_df')\n        # print message\n        print(f\"Saved test metrics to {savefolder}/test_metrics_df\")\n        print(f\"Saved test predictions to {savefolder}/test_pred_df\")\n        return\n    \ndef save_model(cvModel, savepath, overwrite=False):\n    \" save model to databricks \"\n    # Save bestModel\n    print('Saving bestModel...')\n    if not overwrite:\n        try:\n            cvModel.bestModel.write().save(savepath)\n        except:\n            print(\"Save Error: 'overwrite' set to False but file exists.\")\n            return\n    else:\n        cvModel.bestModel.write().overwrite().save(savepath)   \n    print(f\"Saved model to {savepath}\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"19811e8d-5de1-446e-8226-929938ce3521","inputWidgets":{},"title":"MLP SAVE METRICS"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Experiments, Results and Discussion"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b32508ac-e446-4db2-b4a8-fe0d683dedd2","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Logistic Regression Results\n\nMajority of our logistic regression experiments were conducted in Phase 3 during which we found that L1/L2 regularization parameters did not improve performance, undersampling improved skewed F1 scores, and less than 10 iterations were required for the model to converge. Once we have established our baseline, we opted to focus our efforts on the multilayer perceptron classifier in order to benchmark feature impact while fine-tuning the neural network. Below is our Phase 3 results that compare metrics with and without sampling. Without sampling, the average F1 score was 0.900 due to the fact that the major to minor class ratio was 4.87 and the model merely predicted all flights as on-time. With sampling, the average F1 score dropped to 0.523, which appears more reasonable given the AU-ROC and loss curves for the best model. In the interest of conciseness, experiments with deprecated features were omitted. Instead, we obtain a new best logistic regression model using PCA features, events, airport pagerank, airline reutation, airport maintenance percentage, and time of day to compare with the MLPC final model. As you can see in the training curves below, the model quickly converted within 7 iterations. This gave us an indication to lower the maxIteration for our MLPC model as well. Unfortunately, the PR curve did not appear to have good results and AU-ROC was 0.6444.\n\n<img src='https://drive.google.com/uc?id=1wC07UYDyPDNoCd0T7OArXjKidQEuxKcY' alt='Google Drive Image' width=25%/>\n<img src='https://drive.google.com/uc?id=1AbZUVrNX2oDRmdWx2bPi5cSmtAejBh4W' alt='Google Drive Image' width=25%/>\n<img src='https://drive.google.com/uc?id=1gb78Hdlryd3CJxqLvrWIcS_CATIQyGoy' alt='Google Drive Image' width=25%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"22960ac0-fee0-41e3-8036-3128e96edd55","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data = [\n('lr-regParam0-elasticNetParam0-noUndersampling', 10, 4.40, 0, 0, 0.900, '(regParam, [0.0])(elasticNetParam, [0.0])', 'No undersampling'),\n('lr-regParam0-elasticNetParam0-withUndersampling', 10, 4.67, 0.0, 0.0, 0.523, '(regParam, [0.0])(elasticNetParam, [0.0])', 'With undersampling'),\n]\nexperiments_table = pd.DataFrame.from_records(data = data, \n                                              columns = ['filepath', \"cluster size\" ,'training time (min)', 'regParam', 'elasticNetParam', 'Avg F1 Score', 'Param Grid', 'Notes'])\ndisplay(experiments_table)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"ffbb512b-2741-44e7-a821-74aeb878a88b","inputWidgets":{},"title":"Phase 3 Experiments"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["lr-regParam0-elasticNetParam0-noUndersampling",10,4.4,0.0,0.0,0.9,"(regParam, [0.0])(elasticNetParam, [0.0])","No undersampling"],["lr-regParam0-elasticNetParam0-withUndersampling",10,4.67,0.0,0.0,0.523,"(regParam, [0.0])(elasticNetParam, [0.0])","With undersampling"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"filepath","type":"\"string\"","metadata":"{}"},{"name":"cluster size","type":"\"long\"","metadata":"{}"},{"name":"training time (min)","type":"\"double\"","metadata":"{}"},{"name":"regParam","type":"\"double\"","metadata":"{}"},{"name":"elasticNetParam","type":"\"double\"","metadata":"{}"},{"name":"Avg F1 Score","type":"\"double\"","metadata":"{}"},{"name":"Param Grid","type":"\"string\"","metadata":"{}"},{"name":"Notes","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>filepath</th><th>cluster size</th><th>training time (min)</th><th>regParam</th><th>elasticNetParam</th><th>Avg F1 Score</th><th>Param Grid</th><th>Notes</th></tr></thead><tbody><tr><td>lr-regParam0-elasticNetParam0-noUndersampling</td><td>10</td><td>4.4</td><td>0.0</td><td>0.0</td><td>0.9</td><td>(regParam, [0.0])(elasticNetParam, [0.0])</td><td>No undersampling</td></tr><tr><td>lr-regParam0-elasticNetParam0-withUndersampling</td><td>10</td><td>4.67</td><td>0.0</td><td>0.0</td><td>0.523</td><td>(regParam, [0.0])(elasticNetParam, [0.0])</td><td>With undersampling</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["columns = ['CV Best Model', 'Architecture', # string\n           'Test weightedF1', 'Test Balanced Accuracy', 'Train weightedF1', 'Train Balanced Accuracy', # float\n           'CV Scores', # list of float\n           'Training Time (sec)', 'Cluster Size', # float\n           'Parameters', # dict(string: [string])\n           'Features', # list of string\n           'ParamGrid', # dict(string: [string])\n           'Notes'] # string\ndata = [\n('lr_4', 'LR-0.0regParam-0.0elasticNetParam',0.7641114456761416, 0.8178400082244285, 0.6083619582583852, 0.681828808893455,  [0.5710081406869594, 0.5829162346388952, 0.5698614587849208, 0.5666804226088501, 0.6508579117960366], 156.8,10,{'maxIter':['6']},['dest_weather_Avg_HourlyDryBulbTemperature','dest_weather_Avg_HourlyRelativeHumidity','dest_weather_Avg_HourlyVisibility','dest_weather_Sky_Conditions_OVC', 'origin_weather_Avg_HourlyDryBulbTemperature','origin_weather_Avg_HourlyRelativeHumidity','origin_weather_Avg_HourlyVisibility','origin_weather_Sky_Conditions_OVC', \n'time_of_day_OneHot','natural_disaster','PageRank','REPUTATION','MAINT_DELAY_PCT','EVENT_OneHot'],{'maxIter':['6']}, 'Similar to MLP')\n]    \nexperiments_table = pd.DataFrame.from_records(data = data, columns = columns)\ndisplay(experiments_table)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"70263501-1277-4581-a195-a40934acf5e1","inputWidgets":{},"title":"Best Logistic Regression Model (Phase 4)"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["lr_4","LR-0.0regParam-0.0elasticNetParam",0.7641114456761416,0.8178400082244285,0.6083619582583852,0.681828808893455,[0.5710081406869594,0.5829162346388952,0.5698614587849208,0.5666804226088501,0.6508579117960366],156.8,10,{"maxIter":["6"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","time_of_day_OneHot","natural_disaster","PageRank","REPUTATION","MAINT_DELAY_PCT","EVENT_OneHot"],{"maxIter":["6"]},"Similar to MLP"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"CV Best Model","type":"\"string\"","metadata":"{}"},{"name":"Architecture","type":"\"string\"","metadata":"{}"},{"name":"Test weightedF1","type":"\"double\"","metadata":"{}"},{"name":"Test Balanced Accuracy","type":"\"double\"","metadata":"{}"},{"name":"Train weightedF1","type":"\"double\"","metadata":"{}"},{"name":"Train Balanced Accuracy","type":"\"double\"","metadata":"{}"},{"name":"CV Scores","type":"{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":true}","metadata":"{}"},{"name":"Training Time (sec)","type":"\"double\"","metadata":"{}"},{"name":"Cluster Size","type":"\"long\"","metadata":"{}"},{"name":"Parameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true},\"valueContainsNull\":true}","metadata":"{}"},{"name":"Features","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"ParamGrid","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true},\"valueContainsNull\":true}","metadata":"{}"},{"name":"Notes","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CV Best Model</th><th>Architecture</th><th>Test weightedF1</th><th>Test Balanced Accuracy</th><th>Train weightedF1</th><th>Train Balanced Accuracy</th><th>CV Scores</th><th>Training Time (sec)</th><th>Cluster Size</th><th>Parameters</th><th>Features</th><th>ParamGrid</th><th>Notes</th></tr></thead><tbody><tr><td>lr_4</td><td>LR-0.0regParam-0.0elasticNetParam</td><td>0.7641114456761416</td><td>0.8178400082244285</td><td>0.6083619582583852</td><td>0.681828808893455</td><td>List(0.5710081406869594, 0.5829162346388952, 0.5698614587849208, 0.5666804226088501, 0.6508579117960366)</td><td>156.8</td><td>10</td><td>Map(maxIter -> List(6))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, time_of_day_OneHot, natural_disaster, PageRank, REPUTATION, MAINT_DELAY_PCT, EVENT_OneHot)</td><td>Map(maxIter -> List(6))</td><td>Similar to MLP</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Decision Tree Results\n\nMuch like logistic regression, our decision tree model experiments with hyperparameter tuning were mainly conducted in Phase 3, which we observed that the tree was very shallow despite added features and parameters such as minWeightFractionPerNode and maxDepth did not produce any significant improvements to the model. In Phase 4, we ran a decision tree model with PCA-selected features and new engineered features to compare with our MLPC best model. This includes PCA features, events, airport pagerank, airline reutation, airport maintenance percentage, and time of day. Our best model obtained a test weighted F1 score of 0.7506, test balanced accuracy of 0.702, train weighted F1 score of 0.3637, and train balanced accuracy  of 0.833. It took 2.1 minutes and 10 clusters to run the model. Below is a bar graph of feature importance derived from the decision tree model. PageRank, Airport Maintenance Percentage, and Events were all new features that were among the top features. 3 PCA features were also included in the top features. The rest of the features were not used and indicates that future work should involve increasing the correlation of these features by adjusting windows and feature calculations.\n \n \n<img src='https://drive.google.com/uc?id=1F7teRVg6spmjjbGofNI6VOPfqoIX9_zv' alt='Google Drive Image' width=80%/>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f570d667-af21-4392-a590-2dc864f67a1b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# record experiments in a pandas dataframe. Please note that not every file is recorded because there are re-runs.\nexperiments = [\n('dt_1_0.0_joined_df_baseline', 10,11.17,1, 0.0, 32, 0.5997718801586884, '(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])','baseline'),\n('dt_1_0.0_joined_df_new_pagerank',  10,7.36,1,0.0, 32, 0.6028385391617597, '(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])','baseline + pagerank'),\n('df_joined_pagerank_leadingflights',  10,9.49,1, 0.0, 32, 0.5956067766032404, '(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])', 'baseline, pagerank, leadingflights'),\n('df_joined_pagerank_holidaybool', 10,8.37,1, 0.0, 32, 0.5963543260777312, '(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])','baseline, pagerank, holiday'),\n('df_joined_pagerank_holidaybool_leadingflights',  10, 4.86,1, 0.0, 32, 0.5995827909771116,'(maxDepth, [1])(minWeightFractionPerNode, [0.0])','baseline, pagerank, holiday, leadingflights'),\n('df_joined_pagerank_holidaybool_leadingflights_timeOfDay',  10,4.89,1,0.0, 32, 0.36871536716945735,'(maxDepth, [1])(minWeightFractionPerNode, [0.0])', 'baseline, pagerank, holiday, leadingflights, time of day'),\n('df_joined_carrier', 10, 4.51,1, 0.0, 32, 0.6045734194924577,'(maxDepth, [1])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier'),\n('df_joined_carrier_origin', 10,  8.11, 1, 0.0, 32, 0.6011804576678804,'(maxDepth, [1])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier, origin'),\n('df_joined_carrier_distance', 10,6.17,1, 0.0, 32, 0.6010202545477876,'(maxDepth, [1])(minWeightFractionPerNode, [0.0])', 'baseline, op_unique_carrier, distance'),\n( 'df_joined_carrier_pagerank_time', 10, 4.47,1,0.0, 32, 0.5968677938248441,'(maxDepth, [1])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier, pagerank, holiday, leadingflights'),\n('df_joined_carrier_pagerank_time_1', 10, 20.45,4,0.0, 34, 0.6065143280641339,'(maxDepth, [0, 4, 5, 6]),(maxBins, [30, 32, 34])(minInstancesPerNode, [1, 2])(minInfoGain, [0, 0.2])(minWeightFractionPerNode, [0.0, 0.2])','baseline, op_unique_carrier, pagerank, holiday, leadingflights'),\n('df_joined_carrier_pagerank_time_2', 10, 10.68,5, 0.0, 32, 0.611286365026526,'(maxDepth, [0, 2, 3, 4, 5]),(maxBins, [33, 34, 35, 40])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier, pagerank, holiday, leadingflights'),\n('df_joined_carrier_pagerank_time_3', 10, 9.31,6,0.0, 30, 0.6143895428140813,'(maxDepth, [0, 2, 3, 4, 5, 6]),(maxBins, [30, 40])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier, pagerank, holiday, leadingflights'),\n('df_joined_carrier_pagerank_time_4', 10,10.16,6,0.0, 33, 0.6110807714801667,'(maxDepth, [0, 2, 3, 4, 5, 6]),(maxBins, [30, 35, 40, 45])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier, pagerank, holiday, leadingflights'),\n('df_joined_carrier_pagerank_time_5', 10, 11.84,7,0.0, 40, 0.6097127980000007,'(maxDepth, [0, 2, 3, 4, 5, 6, 7]),(maxBins, [30, 40])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier, pagerank, holiday, leadingflights'),\n('df_joined_carrier_pagerank_time_6', 10,6.70,8,0.0, 32, 0.6039963150032969,'(maxDepth, [0, 2, 3, 4, 5, 6, 7, 8]),(maxBins, [32])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier, pagerank, holiday, leadingflights'),\n('df_joined_carrier_pagerank_time_7', 10,9.70,20,0.0, 32, 0.6046275691757348,'(maxDepth, [0, 4, 20]),(maxBins, [32])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier, pagerank, holiday, leadingflights'),\n('df_joined_carrier_pagerank_time_9', 10, 170.0,30,0.0, 32, 0.57,'(maxDepth, [0, 20, 30]),(maxBins, [32])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier, pagerank, holiday, leadingflights'),\n('df_joined_carrier_pagerank_time_9', 10, 17.87,30, 0.0, 32, 0.5752647309493897,'(maxDepth, [0, 15, 30]),(maxBins, [30, 40])(minInstancesPerNode, [2])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier, pagerank, holiday, leadingflights'),#Morgan's\n('df_joined_carrier_pagerank_time_10', 10, 15.63, 20, 0.0, 100, 0.5726972730747718,'(maxDepth, [0, 10, 20]),(maxBins, [100])(minInstancesPerNode, [2])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])','baseline, op_unique_carrier, pagerank, holiday, leadingflights'),\n]\n    \ncolumns = [\"file\", \"cluster_size\", \"training_time\", \"maxDepth\", \"minWeightFractionPerNode\", \"maxBins\", \"avg_f1\", \"param_grid\", \"features\"]\nexperiments_table = pd.DataFrame.from_records(data = experiments, columns=columns)\ndisplay(experiments_table)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"78873613-14bf-4bd3-9140-69e0b168ca83","inputWidgets":{},"title":"Decision Tree Experiments (Phase 3)"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dt_1_0.0_joined_df_baseline",10,11.17,1,0.0,32,0.5997718801586884,"(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])","baseline"],["dt_1_0.0_joined_df_new_pagerank",10,7.36,1,0.0,32,0.6028385391617597,"(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])","baseline + pagerank"],["df_joined_pagerank_leadingflights",10,9.49,1,0.0,32,0.5956067766032404,"(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])","baseline, pagerank, leadingflights"],["df_joined_pagerank_holidaybool",10,8.37,1,0.0,32,0.5963543260777312,"(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])","baseline, pagerank, holiday"],["df_joined_pagerank_holidaybool_leadingflights",10,4.86,1,0.0,32,0.5995827909771116,"(maxDepth, [1])(minWeightFractionPerNode, [0.0])","baseline, pagerank, holiday, leadingflights"],["df_joined_pagerank_holidaybool_leadingflights_timeOfDay",10,4.89,1,0.0,32,0.36871536716945735,"(maxDepth, [1])(minWeightFractionPerNode, [0.0])","baseline, pagerank, holiday, leadingflights, time of day"],["df_joined_carrier",10,4.51,1,0.0,32,0.6045734194924577,"(maxDepth, [1])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier"],["df_joined_carrier_origin",10,8.11,1,0.0,32,0.6011804576678804,"(maxDepth, [1])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, origin"],["df_joined_carrier_distance",10,6.17,1,0.0,32,0.6010202545477876,"(maxDepth, [1])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, distance"],["df_joined_carrier_pagerank_time",10,4.47,1,0.0,32,0.5968677938248441,"(maxDepth, [1])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, pagerank, holiday, leadingflights"],["df_joined_carrier_pagerank_time_1",10,20.45,4,0.0,34,0.6065143280641339,"(maxDepth, [0, 4, 5, 6]),(maxBins, [30, 32, 34])(minInstancesPerNode, [1, 2])(minInfoGain, [0, 0.2])(minWeightFractionPerNode, [0.0, 0.2])","baseline, op_unique_carrier, pagerank, holiday, leadingflights"],["df_joined_carrier_pagerank_time_2",10,10.68,5,0.0,32,0.611286365026526,"(maxDepth, [0, 2, 3, 4, 5]),(maxBins, [33, 34, 35, 40])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, pagerank, holiday, leadingflights"],["df_joined_carrier_pagerank_time_3",10,9.31,6,0.0,30,0.6143895428140813,"(maxDepth, [0, 2, 3, 4, 5, 6]),(maxBins, [30, 40])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, pagerank, holiday, leadingflights"],["df_joined_carrier_pagerank_time_4",10,10.16,6,0.0,33,0.6110807714801667,"(maxDepth, [0, 2, 3, 4, 5, 6]),(maxBins, [30, 35, 40, 45])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, pagerank, holiday, leadingflights"],["df_joined_carrier_pagerank_time_5",10,11.84,7,0.0,40,0.6097127980000007,"(maxDepth, [0, 2, 3, 4, 5, 6, 7]),(maxBins, [30, 40])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, pagerank, holiday, leadingflights"],["df_joined_carrier_pagerank_time_6",10,6.7,8,0.0,32,0.6039963150032969,"(maxDepth, [0, 2, 3, 4, 5, 6, 7, 8]),(maxBins, [32])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, pagerank, holiday, leadingflights"],["df_joined_carrier_pagerank_time_7",10,9.7,20,0.0,32,0.6046275691757348,"(maxDepth, [0, 4, 20]),(maxBins, [32])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, pagerank, holiday, leadingflights"],["df_joined_carrier_pagerank_time_9",10,170.0,30,0.0,32,0.57,"(maxDepth, [0, 20, 30]),(maxBins, [32])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, pagerank, holiday, leadingflights"],["df_joined_carrier_pagerank_time_9",10,17.87,30,0.0,32,0.5752647309493897,"(maxDepth, [0, 15, 30]),(maxBins, [30, 40])(minInstancesPerNode, [2])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, pagerank, holiday, leadingflights"],["df_joined_carrier_pagerank_time_10",10,15.63,20,0.0,100,0.5726972730747718,"(maxDepth, [0, 10, 20]),(maxBins, [100])(minInstancesPerNode, [2])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])","baseline, op_unique_carrier, pagerank, holiday, leadingflights"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"file","type":"\"string\"","metadata":"{}"},{"name":"cluster_size","type":"\"long\"","metadata":"{}"},{"name":"training_time","type":"\"double\"","metadata":"{}"},{"name":"maxDepth","type":"\"long\"","metadata":"{}"},{"name":"minWeightFractionPerNode","type":"\"double\"","metadata":"{}"},{"name":"maxBins","type":"\"long\"","metadata":"{}"},{"name":"avg_f1","type":"\"double\"","metadata":"{}"},{"name":"param_grid","type":"\"string\"","metadata":"{}"},{"name":"features","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>file</th><th>cluster_size</th><th>training_time</th><th>maxDepth</th><th>minWeightFractionPerNode</th><th>maxBins</th><th>avg_f1</th><th>param_grid</th><th>features</th></tr></thead><tbody><tr><td>dt_1_0.0_joined_df_baseline</td><td>10</td><td>11.17</td><td>1</td><td>0.0</td><td>32</td><td>0.5997718801586884</td><td>(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])</td><td>baseline</td></tr><tr><td>dt_1_0.0_joined_df_new_pagerank</td><td>10</td><td>7.36</td><td>1</td><td>0.0</td><td>32</td><td>0.6028385391617597</td><td>(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])</td><td>baseline + pagerank</td></tr><tr><td>df_joined_pagerank_leadingflights</td><td>10</td><td>9.49</td><td>1</td><td>0.0</td><td>32</td><td>0.5956067766032404</td><td>(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])</td><td>baseline, pagerank, leadingflights</td></tr><tr><td>df_joined_pagerank_holidaybool</td><td>10</td><td>8.37</td><td>1</td><td>0.0</td><td>32</td><td>0.5963543260777312</td><td>(maxDepth, [1, 2, 3, 4, 5])(minWeightFractionPerNode, [0.0, 0.1, 0.2, 0.3])</td><td>baseline, pagerank, holiday</td></tr><tr><td>df_joined_pagerank_holidaybool_leadingflights</td><td>10</td><td>4.86</td><td>1</td><td>0.0</td><td>32</td><td>0.5995827909771116</td><td>(maxDepth, [1])(minWeightFractionPerNode, [0.0])</td><td>baseline, pagerank, holiday, leadingflights</td></tr><tr><td>df_joined_pagerank_holidaybool_leadingflights_timeOfDay</td><td>10</td><td>4.89</td><td>1</td><td>0.0</td><td>32</td><td>0.36871536716945735</td><td>(maxDepth, [1])(minWeightFractionPerNode, [0.0])</td><td>baseline, pagerank, holiday, leadingflights, time of day</td></tr><tr><td>df_joined_carrier</td><td>10</td><td>4.51</td><td>1</td><td>0.0</td><td>32</td><td>0.6045734194924577</td><td>(maxDepth, [1])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier</td></tr><tr><td>df_joined_carrier_origin</td><td>10</td><td>8.11</td><td>1</td><td>0.0</td><td>32</td><td>0.6011804576678804</td><td>(maxDepth, [1])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, origin</td></tr><tr><td>df_joined_carrier_distance</td><td>10</td><td>6.17</td><td>1</td><td>0.0</td><td>32</td><td>0.6010202545477876</td><td>(maxDepth, [1])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, distance</td></tr><tr><td>df_joined_carrier_pagerank_time</td><td>10</td><td>4.47</td><td>1</td><td>0.0</td><td>32</td><td>0.5968677938248441</td><td>(maxDepth, [1])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, pagerank, holiday, leadingflights</td></tr><tr><td>df_joined_carrier_pagerank_time_1</td><td>10</td><td>20.45</td><td>4</td><td>0.0</td><td>34</td><td>0.6065143280641339</td><td>(maxDepth, [0, 4, 5, 6]),(maxBins, [30, 32, 34])(minInstancesPerNode, [1, 2])(minInfoGain, [0, 0.2])(minWeightFractionPerNode, [0.0, 0.2])</td><td>baseline, op_unique_carrier, pagerank, holiday, leadingflights</td></tr><tr><td>df_joined_carrier_pagerank_time_2</td><td>10</td><td>10.68</td><td>5</td><td>0.0</td><td>32</td><td>0.611286365026526</td><td>(maxDepth, [0, 2, 3, 4, 5]),(maxBins, [33, 34, 35, 40])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, pagerank, holiday, leadingflights</td></tr><tr><td>df_joined_carrier_pagerank_time_3</td><td>10</td><td>9.31</td><td>6</td><td>0.0</td><td>30</td><td>0.6143895428140813</td><td>(maxDepth, [0, 2, 3, 4, 5, 6]),(maxBins, [30, 40])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, pagerank, holiday, leadingflights</td></tr><tr><td>df_joined_carrier_pagerank_time_4</td><td>10</td><td>10.16</td><td>6</td><td>0.0</td><td>33</td><td>0.6110807714801667</td><td>(maxDepth, [0, 2, 3, 4, 5, 6]),(maxBins, [30, 35, 40, 45])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, pagerank, holiday, leadingflights</td></tr><tr><td>df_joined_carrier_pagerank_time_5</td><td>10</td><td>11.84</td><td>7</td><td>0.0</td><td>40</td><td>0.6097127980000007</td><td>(maxDepth, [0, 2, 3, 4, 5, 6, 7]),(maxBins, [30, 40])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, pagerank, holiday, leadingflights</td></tr><tr><td>df_joined_carrier_pagerank_time_6</td><td>10</td><td>6.7</td><td>8</td><td>0.0</td><td>32</td><td>0.6039963150032969</td><td>(maxDepth, [0, 2, 3, 4, 5, 6, 7, 8]),(maxBins, [32])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, pagerank, holiday, leadingflights</td></tr><tr><td>df_joined_carrier_pagerank_time_7</td><td>10</td><td>9.7</td><td>20</td><td>0.0</td><td>32</td><td>0.6046275691757348</td><td>(maxDepth, [0, 4, 20]),(maxBins, [32])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, pagerank, holiday, leadingflights</td></tr><tr><td>df_joined_carrier_pagerank_time_9</td><td>10</td><td>170.0</td><td>30</td><td>0.0</td><td>32</td><td>0.57</td><td>(maxDepth, [0, 20, 30]),(maxBins, [32])(minInstancesPerNode, [1])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, pagerank, holiday, leadingflights</td></tr><tr><td>df_joined_carrier_pagerank_time_9</td><td>10</td><td>17.87</td><td>30</td><td>0.0</td><td>32</td><td>0.5752647309493897</td><td>(maxDepth, [0, 15, 30]),(maxBins, [30, 40])(minInstancesPerNode, [2])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, pagerank, holiday, leadingflights</td></tr><tr><td>df_joined_carrier_pagerank_time_10</td><td>10</td><td>15.63</td><td>20</td><td>0.0</td><td>100</td><td>0.5726972730747718</td><td>(maxDepth, [0, 10, 20]),(maxBins, [100])(minInstancesPerNode, [2])(minInfoGain, [0])(minWeightFractionPerNode, [0.0])</td><td>baseline, op_unique_carrier, pagerank, holiday, leadingflights</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["columns = ['CV Best Model', 'Architecture', # string\n           'Test weightedF1', 'Test Balanced Accuracy', 'Train weightedF1', 'Train Balanced Accuracy', # float\n           'CV Scores', # list of float\n           'Training Time (sec)', 'Cluster Size', # float\n           'Parameters', # dict(string: [string])\n           'Features', # list of string\n           'ParamGrid', # dict(string: [string])\n           'Notes'] # string\ndata = [\n('dt_1', 'dt_1', 0.7505796667121961, 0.702, 0.3636703685761434, 0.833, [0.5939687403073157, 0.5682450698954058, 0.6109323370682372, 0.5537960533181473, 0.6503336102097778],130.2, 10, {'Default': ['0']}, ['dest_weather_Avg_HourlyDryBulbTemperature','dest_weather_Avg_HourlyRelativeHumidity','dest_weather_Avg_HourlyVisibility','dest_weather_Sky_Conditions_OVC', 'origin_weather_Avg_HourlyDryBulbTemperature','origin_weather_Avg_HourlyRelativeHumidity','origin_weather_Avg_HourlyVisibility','origin_weather_Sky_Conditions_OVC', \n'time_of_day_OneHot',\n'natural_disaster',\n'PageRank',\n'REPUTATION',\n'MAINT_DELAY_PCT',\n'EVENT_OneHot'\n], {'Default': ['0']}, 'decision tree had high training balanced accuracy')\n]    \nexperiments_table = pd.DataFrame.from_records(data = data, columns = columns)\ndisplay(experiments_table)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"1e7593b1-d089-4465-a9ce-04e393b0d52d","inputWidgets":{},"title":"Best Decision Tree Model (Phase 4)"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dt_1","dt_1",0.7505796667121961,0.702,0.3636703685761434,0.833,[0.5939687403073157,0.5682450698954058,0.6109323370682372,0.5537960533181473,0.6503336102097778],130.2,10,{"Default":["0"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","time_of_day_OneHot","natural_disaster","PageRank","REPUTATION","MAINT_DELAY_PCT","EVENT_OneHot"],{"Default":["0"]},"decision tree had high training balanced accuracy"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"CV Best Model","type":"\"string\"","metadata":"{}"},{"name":"Architecture","type":"\"string\"","metadata":"{}"},{"name":"Test weightedF1","type":"\"double\"","metadata":"{}"},{"name":"Test Balanced Accuracy","type":"\"double\"","metadata":"{}"},{"name":"Train weightedF1","type":"\"double\"","metadata":"{}"},{"name":"Train Balanced Accuracy","type":"\"double\"","metadata":"{}"},{"name":"CV Scores","type":"{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":true}","metadata":"{}"},{"name":"Training Time (sec)","type":"\"double\"","metadata":"{}"},{"name":"Cluster Size","type":"\"long\"","metadata":"{}"},{"name":"Parameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true},\"valueContainsNull\":true}","metadata":"{}"},{"name":"Features","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"ParamGrid","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true},\"valueContainsNull\":true}","metadata":"{}"},{"name":"Notes","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CV Best Model</th><th>Architecture</th><th>Test weightedF1</th><th>Test Balanced Accuracy</th><th>Train weightedF1</th><th>Train Balanced Accuracy</th><th>CV Scores</th><th>Training Time (sec)</th><th>Cluster Size</th><th>Parameters</th><th>Features</th><th>ParamGrid</th><th>Notes</th></tr></thead><tbody><tr><td>dt_1</td><td>dt_1</td><td>0.7505796667121961</td><td>0.702</td><td>0.3636703685761434</td><td>0.833</td><td>List(0.5939687403073157, 0.5682450698954058, 0.6109323370682372, 0.5537960533181473, 0.6503336102097778)</td><td>130.2</td><td>10</td><td>Map(Default -> List(0))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, time_of_day_OneHot, natural_disaster, PageRank, REPUTATION, MAINT_DELAY_PCT, EVENT_OneHot)</td><td>Map(Default -> List(0))</td><td>decision tree had high training balanced accuracy</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Multilayer Perceptron Classifier Results\n\n#### Best model\n\nOur baseline multilayer perceptron model consists of 8 inputs (numerical features from PCA), one 4-neuron sigmoid hidden layer, and one 2-neuron softmax output layer. It produced a test weighted F1 score of 0.7494, a test balanced accuracy of 0.5012, a train F1 score of\t0.5534, and a train balanced accuracy of 0.5062. In contrast, our best model has the architecture of 33 inputs, two 4-neuron sigmoid hidden layers, and 2-neuron softmax output layer. The model obtained a test weighted F1 score of 0.7651, a test balanced accuracy of 0.5259, a train F1 score of\t0.6149, and a train balanced accuracy of 0.5450.\n\n<img src='https://drive.google.com/uc?id=1465MWbc07hic9MzvtCpOmGWF-hRA5SV-' alt='Google Drive Image' width=50%/>\n\n\n#### Tuning\n\nOur experiments involve train set sampling, feature engineering, and changing the number of layers and neurons, and hyperparameter tuning max iterations, step size, and solver. Continuing from phase 3, we conducted a simple minor-to-major class ratio undersampling on the major class (on-time flights) to address class imbalance. Without sampling, the logistic regression model in phase 3 predicted all flights to be (on-time) to boost metrics. With sampling, the model is better able to optimize without the interference of class imbalance. Now in phase 4, we attempted to apply SMOTE. The train weighted F1 increased to 0.676 and train balanced accuracy increased to 0.605, which is approximately a 0.05 increase from the baseline for both scores. Unfortunately, we were not able to obtain test weighted F1 and balanced accuracy scores due to issues with aligning our test and train set within the time constraint. Future steps would be to debug our SMOTE functionality and test its impact on the test set. \n\nFeature engineering contributed the most to our score increases, we ran experiments with the baseline PCA features and a single new engineered feature. Most notably, REPUTATION increased test weighted F1 score from out baseline 0.749 to 0.765 and test balanced accuracy from 0.501 to 0.526. Train scores increased more dramatically, with train weighted F1 score increasing from 0.553 to 0.615 and train balanced score from 0.506 to 0.545. Feature engineering's the large contribution to performance improvement and the test scores being higher than the training scores leads us to hypothesize that the model is underfitting. This means that the model is not complex enough and the addition of features adds to model complexity. Thus, much of our experiments with hyperparameters, neurons, and layers aim to increase the complexity of the model. Yet, these adjustments did not make significant impact compared to the effect of feature engineering.\n\nGrid search selected the l-bgs as the better model over gd, holding all other parameters constant. Increasing the step size from 0.01 and decreasing the number of maximum iterations from 40 to 15 did not significantly change train and test scores. This means that the model is stable and has reached convergence at 15 iterations. Using our final set of features, we first experimented with increasing the number of neurons in our single layer from 4 to 7. We hypothesized that increasing the number of neurons in response to the increase in number of inputs (the baseline model has 8 inputs while the final model has 33 inputs) will help retain more information. However, the scores only increased in the magnitude of 3e-4, which is not significant enough to call it an improvement (mlp_5 and mlp_6). Experiments 'mlp_7' increased the number of hidden layers to two, with four neurons each. Compared to our benchmark mlp_5 that has one hidden layer of 4 neurons, mlp_7 only increased test weighted F1 by 7e-4 , test balanced accuracy by 2.2e-3, train weighted F1 by 4.7e-3, and train balanced accuracy by 3.5e-4. An additional trial with an additional 4-neuron layer(mlp_14) did not perform better than our single-hidden layer model. Overall, we hypothesize that class imbalance and feature selection overshadowed the potential of hyperparameter tuning and neural network architecture changes. Nevertheless, there are many non-linear features in our model that may be better-captured using more complex activation functions such as 'relu' as opposed to multilayer perceptron classifier's sigmoid."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b8fbe394-4a08-43cd-9bc0-52f4728c19e9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["mlp_experiments = spark.read.parquet(f\"{blob_url}/MLPv2/experiments_table\")\ndisplay(mlp_experiments)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb82c207-a922-48c1-b61b-d816fe8eddea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["mlp_13","MLP-33input-15sigmoid-4sigmoid-2softmax",0.7644869910947227,0.5235915661409185,0.6227622955256669,0.551434717669387,[0.584966350313345,0.6015082441933514,0.5894952559442107,0.5857979487035645,0.6516614420581811],436.3,10,{"stepSize":[".03"],"maxIter":["30"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","time_of_day_OneHot","natural_disaster","PageRank","REPUTATION","MAINT_DELAY_PCT","EVENT_OneHot"],{"stepSize":["0.03"],"maxIter":["30"]},"Did not improve best score by increase first hidden layer neurons."],["mlp_14","MLP-33input-4sigmoid-4sigmoid-4sigmoid-2softmax",0.7643711642973161,0.5242713748408898,0.6128121369797321,0.5433931414074786,[0.5803110694390823,0.6008848664108406,0.584721971862339,0.5827934239583277,0.651730067982373],471.3,10,{"stepSize":[".03"],"maxIter":["30"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","time_of_day_OneHot","natural_disaster","PageRank","REPUTATION","MAINT_DELAY_PCT","EVENT_OneHot"],{"stepSize":["0.03"],"maxIter":["30"]},"Did not improve best score"],["mlp_4","MLP-8input-4sigmoid-2softmax",0.7493901637283577,0.501159881238278,0.5533857311926049,0.5062018724901025,[0.526928496093271,0.5043961567788007,0.5201283604178017,0.4906859375822105,0.648783075604379],385.1,10,{"stepSize":[".03"],"maxIter":["40"],"solver":["l-bfgs"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC"],{"stepSize":[".03"],"maxIter":["40"],"solver":["l-bfgs"]},"More iterations had about the same results as 10 iterations. BASELINE"],["mlp_7","MLP-33input-4sigmoid-4sigmoid-2softmax",0.7650595284672154,0.5259417646692461,0.6148533373441389,0.5450032030923989,[0.5733197080653226,0.5971442093451799,0.581468068018131,0.5769477115831431,0.6501332485333843],200.4,10,{"stepSize":[".1"],"maxIter":["25"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","time_of_day_OneHot","natural_disaster","PageRank","REPUTATION","MAINT_DELAY_PCT","EVENT_OneHot"],{"stepSize":[".1"],"maxIter":["25"]},"2nd architecture did not improve results."],["mlp_6","MLP-33input-7sigmoid-2softmax",0.7647044687200427,0.5249042260995713,0.6161754764392089,0.5462150588433179,[0.5718465845281916,0.5968069071980014,0.5785349109079769,0.577402770498702,0.651173905226575],271.8,10,{"stepSize":[".1"],"maxIter":["25"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","time_of_day_OneHot","natural_disaster","PageRank","REPUTATION","MAINT_DELAY_PCT","EVENT_OneHot"],{"stepSize":[".1"],"maxIter":["25"]},"All FE - increased neurons to 7 - no improvement."],["mlp_8","MLP-13input-4sigmoid-4sigmoid-2softmax",0.7485059828388734,0.49999999999999994,0.5413838885658487,0.5,[0.557952173640411,0.5369830681749825,0.5423256405659995,0.5199429166609909,0.649324267038511],183.2,10,{"stepSize":[".1"],"maxIter":["25"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","time_of_day_OneHot"],{"stepSize":[".1"],"maxIter":["25"]},"time of day did not improve metric by much"],["mlp_5","MLP-33input-4sigmoid-2softmax",0.764370648353353,0.5237094028979931,0.610195178633864,0.5415236159960332,[0.5703025330550383,0.5923260698872735,0.5746805677780819,0.5733308340476557,0.6512297361481939],87.0,10,{"stepSize":[".03"],"maxIter":["15"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","time_of_day_OneHot","natural_disaster","PageRank","REPUTATION","MAINT_DELAY_PCT","EVENT_OneHot"],{"stepSize":[".03"],"maxIter":["15"]},"All FE - balanced accuracy was poor."],["mlp_3","MLP-8input-4sigmoid-2softmax",0.7493901637283577,0.501159881238278,0.5533857311926049,0.5062018724901025,[0.5148156717317701,0.49016166167259134,0.5230153726210534,0.4627872000807111,0.6490100644248709],252.3,10,{"stepSize":[".03"],"maxIter":["20"],"solver":["l-bfgs"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC"],{"stepSize":[".03"],"maxIter":["20"],"solver":["l-bfgs","gd"]},"l-bfgs is better than gd"],["mlp_9","MLP-9input-4sigmoid-2softmax",0.7497556334973251,0.5018367502088394,0.5635506746785377,0.5109150679589463,[0.5276307715596354,0.5138599270603651,0.5201614161237846,0.47924271671296537,0.6485810394345141],136.4,10,{"stepSize":[".1"],"maxIter":["25"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","natural_disaster"],{"stepSize":[".1"],"maxIter":["25"]},"Improved train scores."],["mlp_12","MLP-9input-4sigmoid-2softmax",0.7531814440287747,0.5058992494332545,0.5710209994051981,0.5152408443495448,[0.45866599418070386,0.5167491092136086,0.5180537751329093,0.5096594055928804,0.6465503455375016],128.6,10,{"stepSize":[".1"],"maxIter":["25"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","REPUTATION"],{"stepSize":[".1"],"maxIter":["25"]},"Slightly train scores."],["mlp_10","MLP-9input-4sigmoid-2softmax",0.749447141525992,0.5013122986114102,0.5579969975684045,0.5080493120362894,[0.574867618686324,0.5118856734548053,0.5135826226017802,0.4720360263378165,0.6487949760130441],107.7,10,{"stepSize":[".1"],"maxIter":["25"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","PageRank"],{"stepSize":[".1"],"maxIter":["25"]},"Slightly train scores."],["mlp_2","MLP-32input-4sigmoid-2softmax",0.0,0.0,0.6755859936618465,0.6058195456561635,[0.5208518635242685,0.5596202176154215,0.561097298970663,0.5445595479212503,0.5831165663074256],1483.0,10,{"stepSize":["0.1"],"maxIter":["10"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC","time_of_day_OneHot","natural_disaster","PageRank","REPUTATION","MAINT_DELAY_PCT","EVENT_OneHot"],{"stepSize":["0.1"],"maxIter":["10"]},"SMOTE"],["mlp_1","MLP-8input-4sigmoid-2softmax",0.7486630007683887,0.5001844014093815,0.5417229911264819,0.5002814837060532,[0.49185105718237043,0.4831412989423932,0.4627117880087465,0.4482906966274149,0.6491596839807433],588.2,10,{"stepSize":["0.1"],"maxIter":["10"]},["dest_weather_Avg_HourlyDryBulbTemperature","dest_weather_Avg_HourlyRelativeHumidity","dest_weather_Avg_HourlyVisibility","dest_weather_Sky_Conditions_OVC","origin_weather_Avg_HourlyDryBulbTemperature","origin_weather_Avg_HourlyRelativeHumidity","origin_weather_Avg_HourlyVisibility","origin_weather_Sky_Conditions_OVC"],{"stepSize":["0.1"],"maxIter":["10"]},"Baseline"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"CV Best Model","type":"\"string\"","metadata":"{}"},{"name":"Architecture","type":"\"string\"","metadata":"{}"},{"name":"Test weightedF1","type":"\"double\"","metadata":"{}"},{"name":"Test Balanced Accuracy","type":"\"double\"","metadata":"{}"},{"name":"Train weightedF1","type":"\"double\"","metadata":"{}"},{"name":"Train Balanced Accuracy","type":"\"double\"","metadata":"{}"},{"name":"CV Scores","type":"{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":true}","metadata":"{}"},{"name":"Training Time (sec)","type":"\"double\"","metadata":"{}"},{"name":"Cluster Size","type":"\"long\"","metadata":"{}"},{"name":"Parameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true},\"valueContainsNull\":true}","metadata":"{}"},{"name":"Features","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"ParamGrid","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true},\"valueContainsNull\":true}","metadata":"{}"},{"name":"Notes","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CV Best Model</th><th>Architecture</th><th>Test weightedF1</th><th>Test Balanced Accuracy</th><th>Train weightedF1</th><th>Train Balanced Accuracy</th><th>CV Scores</th><th>Training Time (sec)</th><th>Cluster Size</th><th>Parameters</th><th>Features</th><th>ParamGrid</th><th>Notes</th></tr></thead><tbody><tr><td>mlp_13</td><td>MLP-33input-15sigmoid-4sigmoid-2softmax</td><td>0.7644869910947227</td><td>0.5235915661409185</td><td>0.6227622955256669</td><td>0.551434717669387</td><td>List(0.584966350313345, 0.6015082441933514, 0.5894952559442107, 0.5857979487035645, 0.6516614420581811)</td><td>436.3</td><td>10</td><td>Map(stepSize -> List(.03), maxIter -> List(30))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, time_of_day_OneHot, natural_disaster, PageRank, REPUTATION, MAINT_DELAY_PCT, EVENT_OneHot)</td><td>Map(stepSize -> List(0.03), maxIter -> List(30))</td><td>Did not improve best score by increase first hidden layer neurons.</td></tr><tr><td>mlp_14</td><td>MLP-33input-4sigmoid-4sigmoid-4sigmoid-2softmax</td><td>0.7643711642973161</td><td>0.5242713748408898</td><td>0.6128121369797321</td><td>0.5433931414074786</td><td>List(0.5803110694390823, 0.6008848664108406, 0.584721971862339, 0.5827934239583277, 0.651730067982373)</td><td>471.3</td><td>10</td><td>Map(stepSize -> List(.03), maxIter -> List(30))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, time_of_day_OneHot, natural_disaster, PageRank, REPUTATION, MAINT_DELAY_PCT, EVENT_OneHot)</td><td>Map(stepSize -> List(0.03), maxIter -> List(30))</td><td>Did not improve best score</td></tr><tr><td>mlp_4</td><td>MLP-8input-4sigmoid-2softmax</td><td>0.7493901637283577</td><td>0.501159881238278</td><td>0.5533857311926049</td><td>0.5062018724901025</td><td>List(0.526928496093271, 0.5043961567788007, 0.5201283604178017, 0.4906859375822105, 0.648783075604379)</td><td>385.1</td><td>10</td><td>Map(stepSize -> List(.03), maxIter -> List(40), solver -> List(l-bfgs))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC)</td><td>Map(stepSize -> List(.03), maxIter -> List(40), solver -> List(l-bfgs))</td><td>More iterations had about the same results as 10 iterations. BASELINE</td></tr><tr><td>mlp_7</td><td>MLP-33input-4sigmoid-4sigmoid-2softmax</td><td>0.7650595284672154</td><td>0.5259417646692461</td><td>0.6148533373441389</td><td>0.5450032030923989</td><td>List(0.5733197080653226, 0.5971442093451799, 0.581468068018131, 0.5769477115831431, 0.6501332485333843)</td><td>200.4</td><td>10</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, time_of_day_OneHot, natural_disaster, PageRank, REPUTATION, MAINT_DELAY_PCT, EVENT_OneHot)</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>2nd architecture did not improve results.</td></tr><tr><td>mlp_6</td><td>MLP-33input-7sigmoid-2softmax</td><td>0.7647044687200427</td><td>0.5249042260995713</td><td>0.6161754764392089</td><td>0.5462150588433179</td><td>List(0.5718465845281916, 0.5968069071980014, 0.5785349109079769, 0.577402770498702, 0.651173905226575)</td><td>271.8</td><td>10</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, time_of_day_OneHot, natural_disaster, PageRank, REPUTATION, MAINT_DELAY_PCT, EVENT_OneHot)</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>All FE - increased neurons to 7 - no improvement.</td></tr><tr><td>mlp_8</td><td>MLP-13input-4sigmoid-4sigmoid-2softmax</td><td>0.7485059828388734</td><td>0.49999999999999994</td><td>0.5413838885658487</td><td>0.5</td><td>List(0.557952173640411, 0.5369830681749825, 0.5423256405659995, 0.5199429166609909, 0.649324267038511)</td><td>183.2</td><td>10</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, time_of_day_OneHot)</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>time of day did not improve metric by much</td></tr><tr><td>mlp_5</td><td>MLP-33input-4sigmoid-2softmax</td><td>0.764370648353353</td><td>0.5237094028979931</td><td>0.610195178633864</td><td>0.5415236159960332</td><td>List(0.5703025330550383, 0.5923260698872735, 0.5746805677780819, 0.5733308340476557, 0.6512297361481939)</td><td>87.0</td><td>10</td><td>Map(stepSize -> List(.03), maxIter -> List(15))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, time_of_day_OneHot, natural_disaster, PageRank, REPUTATION, MAINT_DELAY_PCT, EVENT_OneHot)</td><td>Map(stepSize -> List(.03), maxIter -> List(15))</td><td>All FE - balanced accuracy was poor.</td></tr><tr><td>mlp_3</td><td>MLP-8input-4sigmoid-2softmax</td><td>0.7493901637283577</td><td>0.501159881238278</td><td>0.5533857311926049</td><td>0.5062018724901025</td><td>List(0.5148156717317701, 0.49016166167259134, 0.5230153726210534, 0.4627872000807111, 0.6490100644248709)</td><td>252.3</td><td>10</td><td>Map(stepSize -> List(.03), maxIter -> List(20), solver -> List(l-bfgs))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC)</td><td>Map(stepSize -> List(.03), maxIter -> List(20), solver -> List(l-bfgs, gd))</td><td>l-bfgs is better than gd</td></tr><tr><td>mlp_9</td><td>MLP-9input-4sigmoid-2softmax</td><td>0.7497556334973251</td><td>0.5018367502088394</td><td>0.5635506746785377</td><td>0.5109150679589463</td><td>List(0.5276307715596354, 0.5138599270603651, 0.5201614161237846, 0.47924271671296537, 0.6485810394345141)</td><td>136.4</td><td>10</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, natural_disaster)</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>Improved train scores.</td></tr><tr><td>mlp_12</td><td>MLP-9input-4sigmoid-2softmax</td><td>0.7531814440287747</td><td>0.5058992494332545</td><td>0.5710209994051981</td><td>0.5152408443495448</td><td>List(0.45866599418070386, 0.5167491092136086, 0.5180537751329093, 0.5096594055928804, 0.6465503455375016)</td><td>128.6</td><td>10</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, REPUTATION)</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>Slightly train scores.</td></tr><tr><td>mlp_10</td><td>MLP-9input-4sigmoid-2softmax</td><td>0.749447141525992</td><td>0.5013122986114102</td><td>0.5579969975684045</td><td>0.5080493120362894</td><td>List(0.574867618686324, 0.5118856734548053, 0.5135826226017802, 0.4720360263378165, 0.6487949760130441)</td><td>107.7</td><td>10</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, PageRank)</td><td>Map(stepSize -> List(.1), maxIter -> List(25))</td><td>Slightly train scores.</td></tr><tr><td>mlp_2</td><td>MLP-32input-4sigmoid-2softmax</td><td>0.0</td><td>0.0</td><td>0.6755859936618465</td><td>0.6058195456561635</td><td>List(0.5208518635242685, 0.5596202176154215, 0.561097298970663, 0.5445595479212503, 0.5831165663074256)</td><td>1483.0</td><td>10</td><td>Map(stepSize -> List(0.1), maxIter -> List(10))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC, time_of_day_OneHot, natural_disaster, PageRank, REPUTATION, MAINT_DELAY_PCT, EVENT_OneHot)</td><td>Map(stepSize -> List(0.1), maxIter -> List(10))</td><td>SMOTE</td></tr><tr><td>mlp_1</td><td>MLP-8input-4sigmoid-2softmax</td><td>0.7486630007683887</td><td>0.5001844014093815</td><td>0.5417229911264819</td><td>0.5002814837060532</td><td>List(0.49185105718237043, 0.4831412989423932, 0.4627117880087465, 0.4482906966274149, 0.6491596839807433)</td><td>588.2</td><td>10</td><td>Map(stepSize -> List(0.1), maxIter -> List(10))</td><td>List(dest_weather_Avg_HourlyDryBulbTemperature, dest_weather_Avg_HourlyRelativeHumidity, dest_weather_Avg_HourlyVisibility, dest_weather_Sky_Conditions_OVC, origin_weather_Avg_HourlyDryBulbTemperature, origin_weather_Avg_HourlyRelativeHumidity, origin_weather_Avg_HourlyVisibility, origin_weather_Sky_Conditions_OVC)</td><td>Map(stepSize -> List(0.1), maxIter -> List(10))</td><td>Baseline</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Comparing Models | Successes & Surprises\n\nThe best model was the MLPC Neural Network model with 33 inputs, two 4-neuron sigmoid hidden layers, and 2-neuron softmax output layer. The model obtained a test weighted F1 score of 0.7651, a test balanced accuracy of 0.5259, a train F1 score of\t0.6149, and a train balanced accuracy of 0.5450. We specified the step size to be 0.1 and max iteration to be 25, leaving the rest as default. Our experiments demonstrated that the MLPC model converged quickly - even 10 iterations produced metrics only lower by a magnitude of 1e-3. The potential for hyperparameter tuning to improve the model was largely affected by class imbalance and underfitting, hinted through low train scores and higher test scores. Number of neuron and layers also did not significantly impact model performance. We hypothesized that some features were non-linear and perhaps different activation functions such as relu would be more effective in improving model complexity and addressing underfitting as future work. Feature engineering made the most contribution. Namely, adding airline reputation boosted train and test scores the most. In comparison, logistic regression and decision tree produced similar results when using the same features as the MLPC neural network best model. Logistic Regression's best model converged in 7 iterations without regularization and produced stable scores - test weighted F1 score of 0.764, a test balanced accuracy of 0.818, train weighted F1 score of 0.608 and train balanced accuracy of 0.682. The best decision tree model produced a test weighted F1 score of 0.751 and a test balanced accuracy of 0.702. Although decision tree produced high test balanced accuracy, train weighted F1 score was low (0.364). A surprising observation was that the decision tree feature importance ranking revealed that the model did not use airline reputation in its tree even though it was one of the most important features in the MLPC model.Smote produced as expected large improvement of 0.676 F1 score on the base MLP model when compared to our custom undersampling method with 0.5 F1 score. Early stopping training average score with an independent k fold cross validation with 0.619 F1 score had roughly the same as our expanding window cross validation score of 0.614 with no early stopping. This is suprising how similar they are even though this dataset has time series dependencies."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"250664b5-d2f4-44ad-8932-c1190bff46bf","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Gap Analysis\n\nMajority of the teams on the leaderboard reported F1 scores as their main metric and thus we will be using our F1 score as to benchmark with other teams as well. Scores on the leaderboard ranged from 0.5 to 0.866. Most utilized 6 workers and recent reported training times were approximately 10 minutes, with 10 scores reporting times in the 1-hour range. This is similar to our experiments with 10 clusters and training time of 3 to 7 minutes. In terms of algorithm, many used logistic regression as their baseline with one or more advanced algorithms (random forest, gradient boosted trees, neural network). Some reported \"major surprises\" matched our observations while others contradicted. Like us, many reported that feature engineering significantly boosted performance and prior flight information were very important features. Unlike us, Team Pi-8's decision tree model took the undersampling approach but faced overfitting while their reported recall of 0.66 was similar to our weighted Recall of 0.68. In light of this finding, we recommend trying lower sampling ratios to address underfitting. Team House Spark stood out as the best-performing group with thorough experiments on multiple algorithms. They tested logistic regression, linear regression, random forest, decision tree, gradient boosted tree, and multilayer perceptron classification model. All of their models achieved approximately a test F1 score of 0.865, with the multilayer perceptron model performing the best with an F1 score of 0.882. This is 0.117 higher than our best model's test weighted F1 score. Much like other comments about past flight information, their top features were percent delays in the past day, percent cancellations in the past day, and count flights in the past day. In comparison, our pagerank was partitioned by year, national events on the entrie dataset, and airport maintenance percent in the past year, natural disaster based on weather data from 2 hours ahead of scheduled flight, and airline reputation in the past day. In reflection, airline reputation had the largest impact on our model performance. Thus our recommended next step is to create more features based on flight information as recent as the past day or even 2 hours prior. To summarize recommended steps derived from gap analysis, we recommend creating more features based on flight information from the past day or hour, such as number of scheduled flights, rate of flight increase in the past hour, and cancellations/delays by airline in the past hour. Then, undersampling ratio and SMOTE can be further refined to help improve training scores. Finally, neural network was the right step forward and utilizing tensorflow and pytorch can help create even more sophisticated models that incorporate early-stopping and callback functionalities that make training more seamless than what is provided in MLlib."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e4dd4e3-6fc0-48dc-ab6c-e40f7074721a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Performance & Scalability\n\n\nThroughout the project, our team encountered multiple unforeseen scalability concerns. The first issue was writing to blob after joining the three datasets (airlines, weather, and stations) together. We were unsuccessfully in writing to blob when attempting with the entire dataset at once, and so the solution was to write to blob in piecemeal so that the DAG does not get overwhelmed. Another issue was the cleaniness of the data. As the data has millions of records, having a thorough EDA and truly understanding each column's contents were imperative for scalability. With the data given, there were many nulls and formatting issues we needed resolve before scaling up, otherwise we would lose too much data. Lastly, worker resources were a limiting factor in our work. For the first two phases of the project, we were only given 4 workers, which resulted in high run times for all of our work. In phase three, we were given 10 workers for our jobs, but for many of the models, we used all 10 workers almost all the time, and we still encountered delays and aborts in our pipeline when resources were limited. To remediate these delays, we wrote all of our processes into functions and tried to limit the activity on our cluster to one main focus at a time so that we can have more reasonable runtimes."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c1d08451-8479-4221-891a-86a2c55234a4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Limitations & Future Work\nWe recognize our model is underfitting due to our test scores being higher than our training model scores. When running experiments, the largest impact to improving model perfomance was due to our feature engineering. Future work to further improve our F1-score would be creating additional features to better capture the variation within flight delays. For future work, we also hope to evaluate other models, the impact of improving some ofour exisitng features to be on a 2 hr rolling window, and other methods implement early stopping. \n\nSome features due to limitations in cluster resources, time, and cluster resources were not able to implement on a rolling window and implement on a 2 hr prior to flight basis even though data leakage issues were addressed so no data less than 2 hour window was fed into the model. Pyspark was also limited in the available packages such as XGB model that could integrate with databricks. The MLP model did not have a way to build in early stopping. The pyspark library was also often not very descriptive."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"00206add-d603-4c9b-b34f-ba898d7532ea","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#Conclusion\n\nThe goal of this project is to create a machine learning model that predicts flight departure delays 2 hours ahead of time as a tool for airlines to improve flight scheduling and delay response. Data exploration revealed that flight delays were small abnormalities in the entire dataset and correlations were muted. Thus, we hypothesize that creating new features targeted towards factors causing delay can amplify correlations and improve model performance. To adjust for the class impalance, we implemented a SMOTE technique to synthesize data. The models we focused on were: logistic regression, decision tree and multilayer perceptron classifier. \n\nThe best model was the MLPC Neural Network model with 33 inputs, two 4-neuron sigmoid hidden layers, and 2-neuron softmax output layer with a test weighted F1 score of 0.7651, a test balanced accuracy of 0.5259, a train F1 score of\t0.6149, and a train balanced accuracy of 0.5450. We specified the step size to be 0.1 and max iteration to be 25, leaving the rest as default. The MLPC model converged quickly wihtin 10 iterations and additional neurons and layers lowered metrics. We hypothesized that some features were non-linear and perhaps different activation functions such as relu would be more effective in improving model complexity and addressing underfitting as future work. Feature engineering made the most contribution for all three models. Airline reputation was an important feature in MLPC model while the decision tree model did not use the feature at all. Although decision tree produced high test balanced accuracy, train weighted F1 score was low (0.364), confirming the need for both F1 score and balanced accuracy. \n\nOur results were similar to the majority of the project leaderboard with test weighted F1 scores in the range of 0.72 to 0.78 across all models, using 10 clusters and 3~7 for training time. We also faced similar challenges with severe class imbalance still affecting performance after undersampling in addition to feature engineering making the most contribution to model performance. Team House Spark performed the best with an F1 score of 0.882 and their top features and comments suggest that features engineered using past flight information over a small window (hours before the flight) had a positive affect on model performance.  Throughout the project, we faced severe class imbalance and scalability challenges when writing our datasets to blob storage and conducting EDA calculations. In the future, we recommend creating more features with narrow time-windows and testing more sampling techniques to address severe class imbalance."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"772c74ed-25c2-40f5-85fb-cea2917cbf13","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d68148a7-e078-4627-ab67-42888977e434","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"FP_Section2_Group2_Phase4_MASTER","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":942460095360751}},"nbformat":4,"nbformat_minor":0}
